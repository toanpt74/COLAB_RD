{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toanpt74/COLAB_RD/blob/main/Embedding_Chromadb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "import json\n",
        "import numpy as np\n",
        "import chromadb\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\n",
        "# questions = dataset['question']\n",
        "# print(questions)\n",
        "\n",
        "# df = pd.read_json('data/GPT_Query_20240124.json')\n",
        "# dataset = Dataset.from_pandas(df, preserve_index=False)\n",
        "#\n",
        "# client = chromadb.PersistentClient('DBAssyV3')\n",
        "# collection = client.create_collection('VectorDB_AssyV3')\n",
        "# model = SentenceTransformer('model/sentence-transformers')\n",
        "#\n",
        "# for i in range(len(dataset)):\n",
        "#     collection.add(\n",
        "#         embeddings=[model.encode(f\"{dataset[i]['question']}\").tolist()],\n",
        "#         documents=[dataset[i]['context']],\n",
        "#         metadatas=[{\"answer\":dataset[i][\"answer\"]}],\n",
        "#         ids=[f\"id_{i}\"]\n",
        "#     )\n",
        "\n",
        "#Read data from DB\n",
        "\n",
        "model = SentenceTransformer('model/sentence-transformers')\n",
        "client = chromadb.PersistentClient('DBAssyV3')\n",
        "collection = client.get_collection('VectorDB_AssyV3')\n",
        "\n",
        "#Load DB & Read data from DB\n",
        "# context = collection.query(\n",
        "#   query_embeddings=[model.encode(user_question).tolist()],\n",
        "#   n_results=10\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "def getContext(question, collection, model):\n",
        "    context = collection.query(\n",
        "        query_embeddings=[model.encode(question).tolist()],\n",
        "        n_results=1\n",
        "    )\n",
        "    text = context[\"documents\"][0]\n",
        "    distance = context[\"distances\"][0]\n",
        "    d =np.min(np.array(distance))\n",
        "    answer = context['metadatas'][0]\n",
        "    answer= answer[0]\n",
        "    json_data = json.dumps(answer)\n",
        "    return text[0], d, json_data\n",
        "\n",
        "# model = SentenceTransformer('model/sentence-transformers')\n",
        "# client = chromadb.PersistentClient('DB')\n",
        "# collection = client.get_collection('QueryDB')\n",
        "# user_question=\"What is the production today of the Assy V3 phase?\"\n",
        "# context = getContext(user_question,collection)\n",
        "# print(context)\n",
        "\n",
        "def getQuery(question, context, model, tokenizer):\n",
        "    eval_prompt = f\"\"\"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
        "    You must output the SQL query that answers the question.\n",
        "    ### Input:{question}\n",
        "\n",
        "    ### Context:{context}\n",
        "    ### Response:\n",
        "    \"\"\"\n",
        "    model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        print(tokenizer.decode(model.generate(**model_input, max_new_tokens=1000)[0], skip_special_tokens=True))\n",
        "\n",
        "    return eval_prompt\n",
        "\n",
        "user_question = \"What was the production yesterday of line 14 Assy V3?\"\n",
        "\n",
        "context, d, answer = getContext(user_question, collection, model)\n",
        "\n",
        "#json_data = json.dumps(answer)\n",
        "data = json.loads(answer)\n",
        "print(data[\"answer\"])\n"
      ],
      "metadata": {
        "id": "S2uhKJkZOHsW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}