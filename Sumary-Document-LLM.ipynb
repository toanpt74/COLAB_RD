{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toanpt74/COLAB_RD/blob/main/Sumary-Document-LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://www.dropbox.com/s/vs6ocyvpzzncvwh/new_articles.zip\n",
        "!unzip -q new_articles.zip -d new_articles\n",
        "!pip install -U bitsandbytes transformers peft accelerate trl datasets\n",
        "!pip install  -U langchain\n",
        "!pip install -U chromadb\n",
        "!pip install sentence_transformers\n",
        "\n",
        "import chromadb\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
        "import os,torch\n",
        "\n",
        "#Load data from file in folder\n",
        "loader = DirectoryLoader('/content/new_articles', glob=\"./*.txt\", loader_cls=TextLoader)\n",
        "documents = loader.load()\n",
        "\n",
        "text1 = documents[0]\n",
        "text2 = documents[1]\n",
        "text =text1.page_content + text2.page_content\n",
        "base_model = \"mistralai/Mistral-7B-Instruct-v0.2\" # to try HuggingFaceH4/zephyr-7b-beta\n",
        "\n",
        "# Load base model(Mistral 7B)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit= True,\n",
        "    bnb_4bit_quant_type= \"nf4\",\n",
        "    bnb_4bit_compute_dtype= torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant= False,\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model,\n",
        "        load_in_4bit=True,\n",
        "        quantization_config=bnb_config,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        ")\n",
        "model.config.use_cache = False # silence the warnings. Please re-enable for inference!\n",
        "model.config.pretraining_tp = 1\n",
        "model.gradient_checkpointing_enable()\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
        "tokenizer.padding_side = 'right'\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.add_eos_token = True\n",
        "\n",
        "tokenizer.add_bos_token, tokenizer.add_eos_token\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    max_length=10000,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "llm = HuggingFacePipeline(pipeline = pipeline, model_kwargs = {'temperature':0})\n",
        "template = \"\"\"\n",
        "              Write a summary of the following text delimited by triple backticks.\n",
        "              Return your response which covers the key points of the text.\n",
        "              ```{text}```\n",
        "              SUMMARY:\n",
        "           \"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "context = llm_chain.run(text1)"
      ],
      "metadata": {
        "id": "pra6_FAA6Asw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}