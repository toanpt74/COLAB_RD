{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toanpt74/COLAB_RD/blob/main/Create_Embeding_DB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.text_splitter import TokenTextSplitter\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from pyvi.ViTokenizer import tokenize\n",
        "import os\n",
        "from PyPDF2 import PdfReader\n",
        "import docx\n",
        "\n",
        "from Sentence_Processing import split_into_sentences, split_paragraph_sentence\n",
        "import uuid\n",
        "from chromadb.utils import embedding_functions\n",
        "import pandas as pd\n",
        "\n",
        "os.environ[\"WANDB_MODE\"] =\"offline\"\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "device = \"auto\"\n",
        "model_id = r\"D:\\Tuan\\ChatGPT\\model\\phobert-base-v2\"\n",
        "embedding_fun = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=model_id)\n",
        "model = SentenceTransformer(model_id)\n",
        "\n",
        "def pdf_to_text(file_path):\n",
        "    with open(file_path, \"rb\") as file:\n",
        "        pdf_reader = PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            text += pdf_reader.pages[page_num].extract_text()\n",
        "        return text\n",
        "def word_to_text(file_path):\n",
        "    doc = docx.Document(file_path)\n",
        "    text = \"\"\n",
        "    for paragraph in doc.paragraphs:\n",
        "        line =  paragraph.text.strip()\n",
        "        if len(line)==0: continue # remove empty lines\n",
        "        line = \" \".join(line.split())  # removed duplicated space and \\r \\t \\n\n",
        "        text += line + \"\\n\"\n",
        "    return text\n",
        "\n",
        "def _split_sentences(text):\n",
        "    # Use regular expressions to split the text into sentences based on punctuation followed by whitespace.\n",
        "    sentences = re.split(r'(?<=[.?!])\\s+', text)\n",
        "    return sentences\n",
        "def _combine_sentences(sentences):\n",
        "    # Create a buffer by combining each sentence with its previous and next sentence to provide a wider context.\n",
        "    combined_sentences = []\n",
        "    for i in range(len(sentences)):\n",
        "        combined_sentence = sentences[i]\n",
        "        if i > 0:\n",
        "            combined_sentence = sentences[i-1] + ' ' + combined_sentence\n",
        "        if i < len(sentences) - 1:\n",
        "            combined_sentence += ' ' + sentences[i+1]\n",
        "        combined_sentences.append(combined_sentence)\n",
        "    return combined_sentences\n",
        "def convert_to_vector(texts):\n",
        "    # Try to generate embeddings for a list of texts using a pre-trained model and handle any exceptions.\n",
        "    try:\n",
        "        # response = openai.embeddings.create(\n",
        "        #     input=texts,\n",
        "        #     model=\"text-embedding-3-small\"\n",
        "        # )\n",
        "        embeddings=[]\n",
        "        for sen in texts:\n",
        "            embeding = model.encode(sen)\n",
        "            embeddings.append(embeding)\n",
        "        embeddings = np.array(embeddings)\n",
        "        return embeddings\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred:\", e)\n",
        "        return np.array([])  # Return an empty array in case of an error\n",
        "def _calculate_cosine_distances(embeddings):\n",
        "    # Calculate the cosine distance (1 - cosine similarity) between consecutive embeddings.\n",
        "    distances = []\n",
        "    for i in range(len(embeddings) - 1):\n",
        "        similarity = cosine_similarity([embeddings[i]], [embeddings[i + 1]])[0][0]\n",
        "        distance = 1 - similarity\n",
        "        distances.append(distance)\n",
        "    return distances\n",
        "def chunk_text(data_path):\n",
        "    # Split the input text into individual sentences.\n",
        "    single_sentences_list = split_sentences(data_path)\n",
        "    # Combine adjacent sentences to form a context window around each sentence.\n",
        "    combined_sentences = _combine_sentences(single_sentences_list)\n",
        "    # Convert the combined sentences into vector representations using a neural network model.\n",
        "    embeddings = convert_to_vector(combined_sentences)\n",
        "    # Calculate the cosine distances between consecutive combined sentence embeddings to measure similarity.\n",
        "    distances = _calculate_cosine_distances(embeddings)\n",
        "    breakpoint_percentile_threshold = 50\n",
        "    breakpoint_distance_threshold = np.percentile(distances, breakpoint_percentile_threshold)\n",
        "    # Find all indices where the distance exceeds the calculated threshold, indicating a potential chunk breakpoint.\n",
        "    indices_above_thresh = [i for i, distance in enumerate(distances) if distance > breakpoint_distance_threshold]\n",
        "    # Initialize the list of chunks and a variable to track the start of the next chunk.\n",
        "    chunks = []\n",
        "    start_index = 0\n",
        "    # Loop through the identified breakpoints and create chunks accordingly.\n",
        "    for index in indices_above_thresh:\n",
        "        chunk = ' '.join(single_sentences_list[start_index:index + 1])\n",
        "        chunks.append(chunk)\n",
        "        start_index = index + 1\n",
        "    # If there are any sentences left after the last breakpoint, add them as the final chunk.\n",
        "    if start_index < len(single_sentences_list):\n",
        "        chunk = ' '.join(single_sentences_list[start_index:])\n",
        "        chunks.append(chunk)\n",
        "    # Return the list of text chunks.\n",
        "    return chunks\n",
        "\n",
        "def split_sentences(data_path):\n",
        "    documents_list=[]\n",
        "    for filename in os.listdir(data_path):\n",
        "        print(f'Processing {filename}')\n",
        "        index = 0\n",
        "        ids_list = []\n",
        "        text = \"\"\n",
        "        if filename.endswith('.pdf'):  # pdf files\n",
        "            text = pdf_to_text(os.path.join(data_path, filename))\n",
        "        elif filename.endswith('.docx'):  # doc files\n",
        "            text = word_to_text(os.path.join(data_path, filename))\n",
        "        if text == \"\":\n",
        "            continue  # ignore empty file\n",
        "        sentences = split_into_sentences(text)\n",
        "        for sen in sentences:\n",
        "            documents_list.append(sen)\n",
        "    return documents_list\n",
        "\n",
        "def add_folder_chunk_by_sentence(path):\n",
        "        documents_list = []\n",
        "        ids_list = []\n",
        "\n",
        "        for filename in os.listdir(path):\n",
        "            print(f'Processing {filename} ')\n",
        "            index = 0\n",
        "            text=\"\"\n",
        "            if filename.endswith('.pdf'): #  pdf files\n",
        "                text= pdf_to_text(os.path.join(path, filename))\n",
        "            elif filename.endswith('.docx'): # doc files\n",
        "                text = word_to_text(os.path.join(path, filename))\n",
        "            if text==\"\":\n",
        "                continue #ignore empty file\n",
        "\n",
        "            sentences = split_into_sentences(text)\n",
        "            for sen in sentences:\n",
        "                documents_list.append(sen)\n",
        "                ids_list.append(f\"{filename}_{index}\")\n",
        "                index +=1\n",
        "                print(f\"{len(sen)} \\n {sen}\")\n",
        "            #collection.add(documents=documents_list, ids=ids_list)\n",
        "        return documents_list, ids_list\n",
        "\n",
        "def semantic_search(question, num_chunks_distance=3, num_chunks=3):\n",
        "    collection = client.get_or_create_collection('common', metadata={\"hnsw:space\": \"cosine\"},\n",
        "                                                 embedding_function=embedding_fun)\n",
        "    results = collection.query(\n",
        "         query_texts=question,\n",
        "         include=['distances', 'documents'],\n",
        "         n_results=num_chunks,\n",
        "    )\n",
        "    # print(\"----------------\")\n",
        "    # print(results)\n",
        "    # print(\"----------------\")\n",
        "    distance = results['distances'][0][0]\n",
        "    ids = []\n",
        "    for cur_chunk_id in results[\"ids\"][0]:\n",
        "        cur_chunk_id_index = int(cur_chunk_id.split('_')[-1])\n",
        "        head_id = cur_chunk_id[0:len(cur_chunk_id) - len(str(cur_chunk_id_index))]\n",
        "        for i in range(-num_chunks_distance, num_chunks_distance + 1, 1):\n",
        "            ids.append(head_id + str(cur_chunk_id_index + i))\n",
        "\n",
        "    ids = sorted(set(ids))\n",
        "    results = results if len(ids) == 1 else collection.get(ids=ids)\n",
        "    context = \" \".join(s for s in results[\"documents\"])\n",
        "    return distance, context\n",
        "\n",
        "def createDB():\n",
        "    metadata_list = []\n",
        "    collection = client.get_or_create_collection('common', metadata={\"hnsw:space\": \"cosine\"},\n",
        "                                                 embedding_function=embedding_fun)\n",
        "    documents_list, ids_list= add_folder_chunk_by_sentence(data_path)\n",
        "    for i in range(len(documents_list)):\n",
        "        metadata_list.append({\"type\":\"common\"})\n",
        "    collection.add(documents=documents_list, ids=ids_list,metadatas=metadata_list)\n",
        "    print(\"Done!\")\n",
        "def CreateData8Sys(file):\n",
        "    df = pd.read_json(file)\n",
        "    print(df)\n",
        "    documents_list = []\n",
        "    ids_list = []\n",
        "    metadata_list = []\n",
        "    for i, row in df.iterrows():\n",
        "        id = row[\"ID\"]\n",
        "        ids_list.append(id)\n",
        "        Items = row[\"Items\"]\n",
        "        documents_list.append(Items)\n",
        "        Detail = row[\"Detail\"]\n",
        "        Image_Name = row[\"Image Name\"]\n",
        "        metadata_list.append({\"type\":\"8system\",\"content\":f\"{Detail}\",\"image_path\":f\"{Image_Name}\"})\n",
        "\n",
        "    collection = client.get_or_create_collection('common', metadata={\"hnsw:space\": \"cosine\"},\n",
        "                                                     embedding_function=embedding_fun)\n",
        "    collection.add(documents=documents_list, ids=ids_list, metadatas=metadata_list)\n",
        "    print(\"Done!\")\n",
        "def TestQA(question):\n",
        "    collection = client.get_or_create_collection('common', metadata={\"hnsw:space\": \"cosine\"},\n",
        "                                                 embedding_function=embedding_fun)\n",
        "    results = collection.query(\n",
        "        # query_embeddings=[model.encode(question).tolist()],\n",
        "        query_texts=question,\n",
        "        include=['distances', 'documents','metadatas'],\n",
        "        n_results=5,\n",
        "    )\n",
        "    # #context =\" \".join(s for s in results[\"documents\"][0])\n",
        "    print(results)\n",
        "\n",
        "\n",
        "data_path = r\"D:/Tuan/ChatGPT/SData\"\n",
        "#chunks = chunk_text(data_path)\n",
        "client = chromadb.PersistentClient('QAVectorDB_Vi')\n",
        "\n",
        "filepath =r'D:\\Tuan\\ChatGPT\\data\\8System.json'\n",
        "# createDB()\n",
        "# CreateData8Sys(filepath)\n",
        "# collection = client.get_or_create_collection('common', metadata={\"hnsw:space\": \"cosine\"},\n",
        "#                                                  embedding_function=embedding_fun)\n",
        "# count = collection.count()\n",
        "# print('--------------------------------------')\n",
        "# print(count)\n",
        "\n",
        "# question = \"Tiêu chuẩn sử dụng băng dán Teflon phần nối của ống dẫn/ống khí nén như thế nào?\"\n",
        "# TestQA(question)\n",
        "\n",
        "\n",
        "# kq = semantic_search(question)\n",
        "# print(kq)\n",
        "\n",
        "\n",
        "\n",
        "# question=[\"Samsung Display Vietnam (SDV) được thành lập khi nào?\",\"CEO hiện tại của công ty Samsung Display là ai?\"]\n",
        "# answer =[\"Samsung Display Vietnam (SDV) được thành lập vào ngày 14 tháng 7 năm 2014.\",\"CEO hiện tại của công ty Samsung Display là ông Choi Joo Sun.\"]\n",
        "\n",
        "# question=\"CEO hiện tại của công ty Samsung Display là ai?\"\n",
        "# an1 =\"Technology Innovation Team (Tech Inno Team) được thành lập từ năm 2018, là 1 trong các Team của nhà máy Samsung Display Việt Nam (SDV).\"\n",
        "# an2 =\"Trụ sở chính của Samsung Display là tại Seoul (Hàn Quốc) và CEO hiện tại của công ty Samsung Display là ông Choi Joo Sun.\"\n",
        "# emb1 = model.encode(question).tolist()\n",
        "# emb2 = model.encode(an1).tolist()\n",
        "# emb3 = model.encode(an2).tolist()\n",
        "#\n",
        "# print(emb1)\n",
        "# print(emb2)\n",
        "# print(emb3)\n",
        "#\n",
        "# similarity = cosine_similarity([emb1], [emb2])[0][0]\n",
        "# distance1 = 1 - similarity\n",
        "# similarity = cosine_similarity([emb1], [emb3])[0][0]\n",
        "# distance2 = 1 - similarity\n",
        "# print(distance1)\n",
        "# print(distance2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JKHwOvJxl0Lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t4RckuC1l0b-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y3L8d3zMl0fT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rn97yjzLl0ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Vp7SXWgl0l1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yyNm0RXWl0o-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KcRyqd1Zl0sc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}