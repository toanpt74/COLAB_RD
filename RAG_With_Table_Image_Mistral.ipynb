{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toanpt74/COLAB_RD/blob/main/RAG_With_Table_Image_Mistral.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "https://www.getty.edu/publications/resources/virtuallibrary/0892360224.pdf\n",
        "https://tam159.medium.com/9-methods-to-enhance-the-performance-of-a-llm-rag-application-3bedfdc842e1\n",
        "https://medium.com/@kbouziane.ai/harnessing-rag-for-text-tables-and-images-a-comprehensive-guide-ca4d2d420219\n",
        "\n",
        "from huggingface_hub import login\n",
        "access_token_read = \"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\"\n",
        "\n",
        "login(token = access_token_read)\n",
        "\n",
        "!pip install nltk==3.9.1\n",
        "! pip install langchain langchain-chroma \"unstructured[all-docs]\" pydantic lxml langchainhub\n",
        "!apt-get install poppler-utils\n",
        "!sudo apt install tesseract-ocr\n",
        "!pip install langchain_community\n",
        "!wget https://arxiv.org/pdf/2304.08485.pdf\n",
        "\n",
        "path='/content/'\n",
        "from typing import Any\n",
        "\n",
        "from pydantic import BaseModel\n",
        "from unstructured.partition.pdf import partition_pdf\n",
        "\n",
        "# Get elements\n",
        "raw_pdf_elements = partition_pdf(\n",
        "    filename=path + \"2304.08485.pdf\",\n",
        "    # Using pdf format to find embedded image blocks\n",
        "    extract_images_in_pdf=True,\n",
        "    # Use layout model (YOLOX) to get bounding boxes (for tables) and find titles\n",
        "    # Titles are any sub-section of the document\n",
        "    infer_table_structure=True,\n",
        "    # Post processing to aggregate text once we have the title\n",
        "    chunking_strategy=\"by_title\",\n",
        "    # Chunking params to aggregate text blocks\n",
        "    # Attempt to create a new chunk 3800 chars\n",
        "    # Attempt to keep chunks > 2000 chars\n",
        "    # Hard max on chunks\n",
        "    max_characters=4000,\n",
        "    new_after_n_chars=3800,\n",
        "    combine_text_under_n_chars=2000,\n",
        "    image_output_dir_path=path,\n",
        ")\n",
        "# Create a dictionary to store counts of each type\n",
        "category_counts = {}\n",
        "\n",
        "for element in raw_pdf_elements:\n",
        "    category = str(type(element))\n",
        "    if category in category_counts:\n",
        "        category_counts[category] += 1\n",
        "    else:\n",
        "        category_counts[category] = 1\n",
        "\n",
        "# Unique_categories will have unique elements\n",
        "unique_categories = set(category_counts.keys())\n",
        "category_counts\n",
        "class Element(BaseModel):\n",
        "    type: str\n",
        "    text: Any\n",
        "\n",
        "\n",
        "# Categorize by type\n",
        "categorized_elements = []\n",
        "for element in raw_pdf_elements:\n",
        "    if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
        "        categorized_elements.append(Element(type=\"table\", text=str(element)))\n",
        "    elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
        "        categorized_elements.append(Element(type=\"text\", text=str(element)))\n",
        "\n",
        "# Tables\n",
        "table_elements = [e for e in categorized_elements if e.type == \"table\"]\n",
        "print(len(table_elements))\n",
        "\n",
        "# Text\n",
        "text_elements = [e for e in categorized_elements if e.type == \"text\"]\n",
        "print(len(text_elements))\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, Pipeline\n",
        "import torch\n",
        "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "model = HuggingFacePipeline.from_model_id(\n",
        "    model_id ='mistralai/Mistral-7B-Instruct-v0.2',\n",
        "    task =\"text-generation\",\n",
        "    device=0,\n",
        "    batch_size=2,\n",
        "    model_kwargs={\"temperature\":0, \"torch_dtype\":torch.bfloat16},\n",
        "    pipeline_kwargs={\n",
        "          #\"return_full_text\":True,\n",
        "          \"temperature\":0.5,\n",
        "          \"max_new_tokens\":1000,\n",
        "          \"repetition_penalty\":1.1,\n",
        "          #\"return_dict_in_generate\":True,\n",
        "          },\n",
        ")\n",
        "\n",
        "# Prompt\n",
        "prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text. Give a concise summary of the table or text. Table or text chunk: {element} \"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
        "\n",
        "# Summary chain\n",
        "#model = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
        "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
        "\n",
        "# Apply to tables\n",
        "tables = [i.text for i in table_elements]\n",
        "table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})"
      ],
      "metadata": {
        "id": "QSRK15VRoEgc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}