{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toanpt74/COLAB_RD/blob/main/VAE_PSA_MODEL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import random\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "#from IPython import display\n",
        "import numpy as np\n",
        "import glob\n",
        "import cv2\n",
        "import pandas as pd\n",
        "# set a random seed\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "random.seed(42)\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "# parameters for building the model and training\n",
        "\n",
        "#CV: Width , Height\n",
        "#Tensor: Height, Width\n",
        "\"\"\"\n",
        "Defining Functions\n",
        "\"\"\"\n",
        "COL = 256 #width\n",
        "ROW = 128 #height\n",
        "INPUT_SHAPE=(ROW, COL, 1,)\n",
        "BATCH_SIZE = 128\n",
        "LATENT_DIM = 128\n",
        "def get_dataset(image_dir):\n",
        "    image_file_list = os.listdir(image_dir)\n",
        "\n",
        "    image_paths = [os.path.join(image_dir, fname) for fname in image_file_list]\n",
        "    random.shuffle(image_paths)\n",
        "    train_data = tf.data.Dataset.from_tensor_slices((image_paths))\n",
        "    print(\"Training dataset: {} images\".format(len(image_paths)))\n",
        "    return train_data, len(image_paths)\n",
        "\n",
        "\n",
        "def pre_image(image_filename):\n",
        "    img_raw = tf.io.read_file(image_filename)\n",
        "    #img_raw = tf.io.read_file(r'E:\\1.Project\\6.PSA\\Left\\Images\\Label\\A3DV1S42ANQBC09620240531112039_L.bmp')\n",
        "    #print('read file :', image_filename)\n",
        "    #image = tf.image.decode_image(img_raw)\n",
        "    #image = tf.cast(image, dtype=tf.float32)\n",
        "\n",
        "    #image = tf.image.resize(image, (COL, ROW))\n",
        "    image = tf.image.decode_image(img_raw, expand_animations=False)\n",
        "    image = tf.cast(image, dtype=tf.float32)\n",
        "\n",
        "    image = tf.image.resize(image, [ROW, COL], method='nearest')\n",
        "    image = image / 255.0\n",
        "    image = tf.reshape(image, shape=(ROW, COL, 1,))\n",
        "    print(image.shape)\n",
        "    # cv2.imshow('',image.numpy())\n",
        "    return image\n",
        "\n",
        "class Sampling(layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        mu, sigma = inputs\n",
        "        batch = tf.shape(mu)[0]\n",
        "        dim = tf.shape(mu)[1]\n",
        "        epsilon = keras.backend.random_normal(shape=(batch, dim))\n",
        "        z = mu + tf.exp(0.5 * sigma) * epsilon\n",
        "        # z = mu + tf.exp(0.5 * sigma)\n",
        "        return z\n",
        "def encoder_conv_block(inputs,filter):\n",
        "    e1 = layers.Conv2D(filters=2**filter, kernel_size=3, strides=(1,1), padding=\"same\",dilation_rate=(1,1), activation='relu')(inputs)\n",
        "    e1 = layers.BatchNormalization()(e1)\n",
        "\n",
        "    e2 = layers.Conv2D(filters=2**filter+1, kernel_size=3, strides=(1,1), padding=\"same\", dilation_rate=(2, 2), activation='relu')(inputs)\n",
        "    e2 = layers.BatchNormalization()(e2)\n",
        "\n",
        "    e = layers.concatenate(inputs=[e1, e2], axis=-1)\n",
        "    y = layers.Conv2D(filters=2**filter+1, kernel_size=3, strides=(2,2), padding=\"same\", dilation_rate=(1, 1), activation='relu')(e)\n",
        "    return y\n",
        "\n",
        "# def encoder_layers(inputs, latent_dim):\n",
        "#     x = layers.Conv2D(filters=4, kernel_size=3, strides=2, padding=\"same\", activation='relu')(inputs)\n",
        "#     x = layers.BatchNormalization()(x)\n",
        "#\n",
        "#     #batuan\n",
        "#     # x = layers.Conv2D(filters=8, kernel_size=3, strides=2, padding='same', activation='relu')(x)\n",
        "#     # x = layers.BatchNormalization()(x)\n",
        "#     # batuan\n",
        "#\n",
        "#     x = layers.Conv2D(filters=16, kernel_size=3, strides=2, padding='same', activation='relu')(x)\n",
        "#     x = layers.BatchNormalization()(x)\n",
        "#     # x = layers.Flatten()(x)\n",
        "#\n",
        "#     x = layers.Conv2D(filters=32, kernel_size=3, strides=2, padding='same', activation='relu')(x)\n",
        "#     batch_3 = layers.BatchNormalization()(x)\n",
        "#\n",
        "#     x = layers.Flatten()(batch_3)\n",
        "#\n",
        "#     mu = layers.Dense(latent_dim, name='latent_mu')(x)\n",
        "#     sigma = layers.Dense(latent_dim, name='latent_sigma')(x)\n",
        "#     return mu, sigma, batch_3.shape\n",
        "\n",
        "# def encoder_layers(inputs, latent_dim):\n",
        "#     x = encoder_conv_block(inputs=inputs,filter=6)\n",
        "#     x = layers.BatchNormalization()(x)\n",
        "#\n",
        "#     # x = layers.Conv2D(filters=8, kernel_size=3, strides=2, padding='same', activation='relu')(x)\n",
        "#     # x = layers.BatchNormalization()(x)\n",
        "#\n",
        "#     y = encoder_conv_block(inputs=x,filter=7)\n",
        "#     z = layers.concatenate([x, y], name=\"sum\",axis=1 )\n",
        "#     z = layers.BatchNormalization()(z)\n",
        "#\n",
        "#     z = encoder_conv_block(inputs=x,filter=8)\n",
        "#     batch_3 = layers.BatchNormalization()(z)\n",
        "#     z = layers.Flatten()(batch_3)\n",
        "#     mu = layers.Dense(latent_dim, name='latent_mu')(z)\n",
        "#     sigma = layers.Dense(latent_dim, name='latent_sigma')(z)\n",
        "#     return mu, sigma, batch_3.shape\n",
        "def encoder_layers(inputs, latent_dim):\n",
        "    x = encoder_conv_block(inputs=inputs,filter=3)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # x = layers.Conv2D(filters=8, kernel_size=3, strides=2, padding='same', activation='relu')(x)\n",
        "    # x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = encoder_conv_block(inputs=x,filter=4)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = encoder_conv_block(inputs=x,filter=5)\n",
        "    batch_3 = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Flatten()(batch_3)\n",
        "\n",
        "    mu = layers.Dense(latent_dim, name='latent_mu')(x)\n",
        "    sigma = layers.Dense(latent_dim, name='latent_sigma')(x)\n",
        "    return mu, sigma, batch_3.shape\n",
        "\n",
        "\n",
        "def encoder_model(latent_dim, input_shape):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    mu, sigma, conv_shape = encoder_layers(inputs, latent_dim=latent_dim)\n",
        "    z = Sampling()((mu, sigma))\n",
        "    model = keras.Model(inputs, outputs=[mu, sigma, z], name='Encoder')\n",
        "    model.summary()\n",
        "    # keras.utils.plot_model(\n",
        "    #     model,\n",
        "    #     to_file='encoder.png',\n",
        "    #     show_shapes=True,\n",
        "    #     show_layer_names=True\n",
        "    # )\n",
        "    return model, conv_shape\n",
        "\n",
        "def decoder_layers(inputs, conv_shape):\n",
        "    units = conv_shape[1] * conv_shape[2] * conv_shape[3]\n",
        "    x = layers.Dense(units, activation='relu')(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Reshape((conv_shape[1], conv_shape[2], conv_shape[3]))(x)\n",
        "\n",
        "    x = layers.Conv2DTranspose(filters=32, kernel_size=3, strides=2, padding='same', activation='relu')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Conv2DTranspose(filters=16, kernel_size=3, strides=2, padding='same', activation='relu')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # x = layers.Conv2DTranspose(filters=8, kernel_size=3, strides=2, padding='same', activation='relu')(x)\n",
        "    # x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Conv2DTranspose(filters=4, kernel_size=3, strides=2, padding='same', activation='relu')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Conv2DTranspose(filters=1, kernel_size=3, strides=1, padding='same', activation='sigmoid')(x)\n",
        "    return x\n",
        "def decoder_model(latent_dim, conv_shape):\n",
        "    inputs = layers.Input(shape=(latent_dim,))\n",
        "    outputs = decoder_layers(inputs, conv_shape)\n",
        "    model = keras.Model(inputs, outputs, name='Decoder')\n",
        "    model.summary()\n",
        "    # keras.utils.plot_model(\n",
        "    #     model,\n",
        "    #     to_file='decoder.png',\n",
        "    #     show_shapes=True,\n",
        "    #     show_layer_names=True\n",
        "    # )\n",
        "    return model\n",
        "def kl_reconstruction_loss(inputs, outputs, mu, sigma):\n",
        "    kl_loss = 1 + sigma - tf.square(mu) - tf.math.exp\n",
        "\n",
        "def kl_reconstruction_loss(inputs, outputs, mu, sigma):\n",
        "    kl_loss = 1 + sigma - tf.square(mu) - tf.math.exp(sigma)\n",
        "    return tf.reduce_mean(kl_loss) * -0.5\n",
        "def vae_model(encoder, decoder, input_shape):\n",
        "    inputs = keras.layers.Input(shape=input_shape)\n",
        "    mu, sigma, z = encoder(inputs)\n",
        "    reconstructed = decoder(z)\n",
        "    model = keras.Model(inputs=inputs, outputs=reconstructed)\n",
        "    loss = kl_reconstruction_loss(inputs, z, mu, sigma)\n",
        "    model.add_loss(loss)\n",
        "    return model\n",
        "\n",
        "def get_vae_models(input_shape, latent_dim):\n",
        "    encoder, conv_shape = encoder_model(latent_dim=latent_dim, input_shape=input_shape)\n",
        "    decoder = decoder_model(latent_dim=latent_dim, conv_shape=conv_shape)\n",
        "    vae = vae_model(encoder, decoder, input_shape=input_shape)\n",
        "    return encoder, decoder, vae\n",
        "\n",
        "\n",
        "def generate_and_save_images(model, epoch, step, test_input):\n",
        "    predictions = model.predict(test_input)\n",
        "    fig = plt.figure(figsize=(4, 4))\n",
        "    for i in range(predictions.shape[0]):\n",
        "        plt.subplot(4, 4, i + 1)\n",
        "        img = predictions[i, :, :, 0] * 255\n",
        "        img = img.astype('int32')\n",
        "        plt.imshow(img, cmap='gray')\n",
        "        plt.axis('off')\n",
        "\n",
        "    fig.suptitle(\"epoch: {}, step: {}\".format(epoch, step))\n",
        "    plt.savefig('image_at_epoch_{:04d}_step{:04d}.png'.format(epoch, step))\n",
        "    fig.clear()\n",
        "    plt.close(fig)\n",
        "    # plt.show()\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "VAE model\n",
        "\"\"\"\n",
        "\n",
        "def Train_VAE(datapath=\"\", epochs=30000, use_transferlearning=False, model_path=\"\"):\n",
        "    if use_transferlearning:\n",
        "        vae = tf.keras.models.load_model(model_path)\n",
        "\n",
        "    else:\n",
        "        encoder, decoder, vae = get_vae_models(input_shape=INPUT_SHAPE, latent_dim=LATENT_DIM)\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=0.0005)\n",
        "    loss_metric = keras.metrics.Mean()\n",
        "    mse_loss = keras.losses.MeanSquaredError()\n",
        "\n",
        "    '''\n",
        "    Preparing dataset\n",
        "    '''\n",
        "    train_data, no_train = get_dataset(datapath)\n",
        "    train_ds = (train_data\n",
        "                .shuffle(no_train)\n",
        "                .map(pre_image, num_parallel_calls=AUTO)\n",
        "                .batch(BATCH_SIZE)\n",
        "                .prefetch(buffer_size=AUTO))\n",
        "\n",
        "    '''\n",
        "    Training loop\n",
        "    '''\n",
        "    # random_vector_for_generation = tf.random.normal(shape=[16, LATENT_DIM])\n",
        "    # generate_and_save_images(decoder, 0, 0, random_vector_for_generation)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print('Start of epoch %d' % (epoch,))\n",
        "        for step, x_batch_train in enumerate(train_ds):\n",
        "            print(f\"SHAPE= {x_batch_train.shape}\")\n",
        "            with tf.GradientTape() as tape:\n",
        "                reconstructed = vae(x_batch_train)\n",
        "                flattened_inputs = tf.reshape(x_batch_train, shape=[-1])\n",
        "                flattened_outputs = tf.reshape(reconstructed, shape=[-1])\n",
        "\n",
        "                loss = mse_loss(flattened_inputs, flattened_outputs) * COL * ROW\n",
        "                loss += sum(vae.losses)\n",
        "\n",
        "            grads = tape.gradient(loss, vae.trainable_weights)\n",
        "            optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
        "\n",
        "            loss_metric(loss)\n",
        "            print('Epoch: %s step: %s mean loss = %s' % (epoch, step, loss_metric.result().numpy()))\n",
        "        if epoch % 100 == 0:\n",
        "            vae.save(os.path.join(model_path,f'PSA_Vae_TAB1_L4__{epoch}__{loss}'),\n",
        "                     save_format=\"tf\")\n",
        "\n",
        "# datapath = r'E:\\ToanPT\\1.Code_train_Unet\\data\\PSA\\DATA-TRAIN\\LINE 4\\TRAIN\\TAB6'\n",
        "# Train_VAE(datapath=datapath,epochs=10001,use_transferlearning = False,\n",
        "#           model_path=r\"E:\\VideoClassification\\model\\PSA\\LINE4\\VAE\\VAE-TAB6-NEW-L4\")\n",
        "\n",
        "\n",
        "print(\"DONE\")\n",
        "\n",
        "#Test model\n",
        "def mse(imageA, imageB):\n",
        "    # the 'Mean Squared Error' between the two images is the\n",
        "    # sum of the squared difference between the two images;\n",
        "    # NOTE: the two images must have the same dimension\n",
        "    err = np.sum((imageA.astype(\"float\") - imageB.astype(\"float\")) ** 2)\n",
        "    err /= float(imageA.shape[0] * imageA.shape[1])\n",
        "\n",
        "    # return the MSE, the lower the error, the more \"similar\"\n",
        "    # the two images are\n",
        "    return err\n",
        "\n",
        "\n",
        "model_path=r'E:\\VideoClassification\\model\\PSA\\LINE4\\VAE\\VAE-TAB1-NEW-L4\\PSA_Vae_TAB1_L4__10000__3.9850244522094727'\n",
        "vae = tf.keras.models.load_model(model_path)\n",
        "\n",
        "def Predict(file):\n",
        "    #vae = tf.keras.models.load_model(model_path)\n",
        "    img = cv2.imread(file,0)\n",
        "    img = cv2.resize(img, (COL, ROW))\n",
        "    x = img.astype('float32') / 255.0\n",
        "    vae_img = x.reshape((1, ROW, COL, 1))\n",
        "    x_recon = vae.predict(vae_img)\n",
        "    x_recon=x_recon.reshape(ROW,COL)\n",
        "    x_input = vae_img.reshape(ROW, COL)\n",
        "    mse_error = mse(x_input, x_recon)\n",
        "    return mse_error\n",
        "\n",
        "def Eval():\n",
        "    NG=0\n",
        "    OK=0\n",
        "    dir_predict=r'E:\\ToanPT\\1.Code_train_Unet\\data\\PSA\\DATA-TRAIN\\LINE 4\\NG\\LEFT\\TAB1'\n",
        "    image_file_list = os.listdir(dir_predict)\n",
        "    image_paths = [os.path.join(dir_predict, fname) for fname in image_file_list]\n",
        "    n = len(image_paths)\n",
        "    R = np.zeros(n)\n",
        "    print(f\"Number Images ={n}\")\n",
        "    for i in range(n):\n",
        "        e1 = Predict(image_paths[i])\n",
        "        if e1 < 0.00018:\n",
        "            OK =OK +1\n",
        "        elif e1 > 0.002:\n",
        "            NG= NG+1\n",
        "        R[i] = e1\n",
        "\n",
        "    print(f\"MAX= {np.max(R)}\")\n",
        "    print(f\"MIN= {np.min(R)}\")\n",
        "    print(f\"STD= {np.std(R)}\")\n",
        "    print(f\"AVG={np.average(R)}\")\n",
        "    print(f\"OK={OK}, NG={NG}\")\n",
        "\n",
        "    # df = pd.DataFrame({\"Value\": R})\n",
        "    # df.to_excel(r\"D:\\Private_Documents\\VAE-PSA-TAB1.xlsx\", index=False)\n",
        "\n",
        "    # e1 = Predict(model_path, image_paths[0])\n",
        "    # print(e1)\n",
        "\n",
        "Eval()\n",
        "\n",
        "# mse = np.mean(np.power(x_test - x_test_decoded,2)m axis=1)\n",
        "# threshold = np.max(mse)"
      ],
      "metadata": {
        "id": "JKHwOvJxl0Lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t4RckuC1l0b-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y3L8d3zMl0fT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rn97yjzLl0ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Vp7SXWgl0l1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yyNm0RXWl0o-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KcRyqd1Zl0sc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}