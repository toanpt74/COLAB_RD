{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toanpt74/COLAB_RD/blob/main/DQN_GridWorld.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import operator\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class GridWorld:\n",
        "    ## Initialise starting data\n",
        "    def __init__(self):\n",
        "        # Set information about the gridworld\n",
        "        self.height = 5\n",
        "        self.width = 5\n",
        "        self.grid = np.zeros((self.height, self.width)) - 1\n",
        "        # Set random start location for the agent\n",
        "        self.current_location = (4, np.random.randint(0, 5))\n",
        "        # Set locations for the bomb and the gold\n",
        "        self.bomb_location = (1, 3)\n",
        "        self.gold_location = (0, 3)\n",
        "        self.terminal_states = [self.bomb_location, self.gold_location]\n",
        "        # Set grid rewards for special cells\n",
        "        self.grid[self.bomb_location[0], self.bomb_location[1]] = -10\n",
        "        self.grid[self.gold_location[0], self.gold_location[1]] = 10\n",
        "        # Set available actions\n",
        "        self.actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
        "    ## Put methods here:\n",
        "    def get_available_actions(self):\n",
        "        \"\"\"Returns possible actions\"\"\"\n",
        "        return self.actions\n",
        "    def agent_on_map(self):\n",
        "        \"\"\"Prints out current location of the agent on the grid (used for debugging)\"\"\"\n",
        "        grid = np.zeros((self.height, self.width))\n",
        "        grid[self.current_location[0], self.current_location[1]] = 1\n",
        "        return grid\n",
        "    def get_reward(self, new_location):\n",
        "        \"\"\"Returns the reward for an input position\"\"\"\n",
        "        return self.grid[new_location[0], new_location[1]]\n",
        "    def make_step(self, action):\n",
        "        \"\"\"Moves the agent in the specified direction. If agent is at a border, agent stays still\n",
        "        but takes negative reward. Function returns the reward for the move.\"\"\"\n",
        "        # Store previous location\n",
        "        last_location = self.current_location\n",
        "        # UP\n",
        "        if action == 'UP':\n",
        "            # If agent is at the top, stay still, collect reward\n",
        "            if last_location[0] == 0:\n",
        "                reward = self.get_reward(last_location)\n",
        "            else:\n",
        "                self.current_location = (self.current_location[0] - 1, self.current_location[1])\n",
        "                reward = self.get_reward(self.current_location)\n",
        "        # DOWN\n",
        "        elif action == 'DOWN':\n",
        "            # If agent is at bottom, stay still, collect reward\n",
        "            if last_location[0] == self.height - 1:\n",
        "                reward = self.get_reward(last_location)\n",
        "            else:\n",
        "                self.current_location = (self.current_location[0] + 1, self.current_location[1])\n",
        "                reward = self.get_reward(self.current_location)\n",
        "\n",
        "        # LEFT\n",
        "        elif action == 'LEFT':\n",
        "            # If agent is at the left, stay still, collect reward\n",
        "            if last_location[1] == 0:\n",
        "                reward = self.get_reward(last_location)\n",
        "            else:\n",
        "                self.current_location = (self.current_location[0], self.current_location[1] - 1)\n",
        "                reward = self.get_reward(self.current_location)\n",
        "\n",
        "        # RIGHT\n",
        "        elif action == 'RIGHT':\n",
        "            # If agent is at the right, stay still, collect reward\n",
        "            if last_location[1] == self.width - 1:\n",
        "                reward = self.get_reward(last_location)\n",
        "            else:\n",
        "                self.current_location = (self.current_location[0], self.current_location[1] + 1)\n",
        "                reward = self.get_reward(self.current_location)\n",
        "\n",
        "        return reward\n",
        "\n",
        "    def check_state(self):\n",
        "        \"\"\"Check if the agent is in a terminal state (gold or bomb), if so return 'TERMINAL'\"\"\"\n",
        "        if self.current_location in self.terminal_states:\n",
        "            return 'TERMINAL'\n",
        "\n",
        "class RandomAgent():\n",
        "    # Choose a random action\n",
        "    def choose_action(self, available_actions):\n",
        "        \"\"\"Returns a random choice of the available actions\"\"\"\n",
        "        return np.random.choice(available_actions)\n",
        "\n",
        "\n",
        "class Q_Agent():\n",
        "    # Intialise\n",
        "    def __init__(self, environment, epsilon=0.05, alpha=0.1, gamma=1):\n",
        "        self.environment = environment\n",
        "        self.q_table = dict()  # Store all Q-values in dictionary of dictionaries\n",
        "        for x in range(environment.height):  # Loop through all possible grid spaces, create sub-dictionary for each\n",
        "            for y in range(environment.width):\n",
        "                self.q_table[(x, y)] = {'UP': 0, 'DOWN': 0, 'LEFT': 0,\n",
        "                                        'RIGHT': 0}  # Populate sub-dictionary with zero values for possible moves\n",
        "\n",
        "        self.epsilon = epsilon\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def choose_action(self, available_actions):\n",
        "        \"\"\"Returns the optimal action from Q-Value table. If multiple optimal actions, chooses random choice.\n",
        "        Will make an exploratory random action dependent on epsilon.\"\"\"\n",
        "        if np.random.uniform(0, 1) < self.epsilon:\n",
        "            action = available_actions[np.random.randint(0, len(available_actions))]\n",
        "        else:\n",
        "            q_values_of_state = self.q_table[self.environment.current_location]\n",
        "            maxValue = max(q_values_of_state.values())\n",
        "            action = np.random.choice([k for k, v in q_values_of_state.items() if v == maxValue])\n",
        "\n",
        "        return action\n",
        "\n",
        "    def learn(self, old_state, reward, new_state, action):\n",
        "        \"\"\"Updates the Q-value table using Q-learning\"\"\"\n",
        "        q_values_of_state = self.q_table[new_state]\n",
        "        max_q_value_in_new_state = max(q_values_of_state.values())\n",
        "        current_q_value = self.q_table[old_state][action]\n",
        "\n",
        "        self.q_table[old_state][action] = (1 - self.alpha) * current_q_value + self.alpha * (\n",
        "                    reward + self.gamma * max_q_value_in_new_state)\n",
        "\n",
        "\n",
        "def play(environment, agent, trials=500, max_steps_per_episode=1000, learn=False):\n",
        "    \"\"\"The play function runs iterations and updates Q-values if desired.\"\"\"\n",
        "    reward_per_episode = []  # Initialise performance log\n",
        "\n",
        "    for trial in range(trials):  # Run trials\n",
        "        cumulative_reward = 0  # Initialise values of each game\n",
        "        step = 0\n",
        "        game_over = False\n",
        "        while step < max_steps_per_episode and game_over != True:  # Run until max steps or until game is finished\n",
        "            old_state = environment.current_location\n",
        "            action = agent.choose_action(environment.actions)\n",
        "            reward = environment.make_step(action)\n",
        "            new_state = environment.current_location\n",
        "\n",
        "            if learn == True:  # Update Q-values if learning is specified\n",
        "                agent.learn(old_state, reward, new_state, action)\n",
        "\n",
        "            cumulative_reward += reward\n",
        "            step += 1\n",
        "\n",
        "            if environment.check_state() == 'TERMINAL':  # If game is in terminal state, game over and start next trial\n",
        "                environment.__init__()\n",
        "                game_over = True\n",
        "\n",
        "        reward_per_episode.append(cumulative_reward)  # Append reward for current trial to performance log\n",
        "\n",
        "    return reward_per_episode  # Return performance log\n",
        "\n",
        "# env = GridWorld()\n",
        "# agent = Q_Agent(env)\n",
        "#\n",
        "# print(\"Current position of the agent =\", env.current_location)\n",
        "# print(env.agent_on_map())\n",
        "# available_actions = env.get_available_actions()\n",
        "# print(\"Available_actions =\", available_actions)\n",
        "# chosen_action = agent.choose_action(available_actions)\n",
        "# print(\"Randomly chosen action =\", chosen_action)\n",
        "# reward = env.make_step(chosen_action)\n",
        "# print(\"Reward obtained =\", reward)\n",
        "# print(\"Current position of the agent =\", env.current_location)\n",
        "# print(env.agent_on_map())\n",
        "# environment = GridWorld()\n",
        "# agentQ = Q_Agent(environment)\n",
        "# # Note the learn=True argument!\n",
        "# reward_per_episode = play(environment, agentQ, trials=500, learn=True)\n",
        "# # Simple learning curve\n",
        "# plt.plot(reward_per_episode)\n",
        "# plt.show()\n",
        "# print(reward_per_episode)\n",
        "# def pretty(d, indent=0):\n",
        "#     for key, value in d.items():\n",
        "#         print('\\t' * indent + str(key))\n",
        "#         if isinstance(value, dict):\n",
        "#             pretty(value, indent+1)\n",
        "#         else:\n",
        "#             print('\\t' * (indent+1) + str(value))\n",
        "# print(agentQ.q_table)\n",
        "\n",
        "\n",
        "def randPair(s,e):\n",
        "    return np.random.randint(s,e), np.random.randint(s,e)\n",
        "\n",
        "#finds an array in the \"depth\" dimension of the grid\n",
        "def findLoc(state, obj):\n",
        "    for i in range(0,4):\n",
        "        for j in range(0,4):\n",
        "            if (state[i,j] == obj).all():\n",
        "                return i,j\n",
        "\n",
        "def initGridPlayer():\n",
        "    state = np.zeros((4,4,4))\n",
        "\n",
        "    #place player\n",
        "    x, y = randPair(0,4)\n",
        "    #state[randPair(0,4)] = np.array([0,0,0,1])\n",
        "    state[x,y] = np.array([0, 0, 0, 1])\n",
        "    #place wall\n",
        "    state[2,2] = np.array([0,0,1,0])\n",
        "    #place pit\n",
        "    state[1,1] = np.array([0,1,0,0])\n",
        "    #place goal\n",
        "    state[1,2] = np.array([1,0,0,0])\n",
        "    a = findLoc(state, np.array([0,0,0,1])) #find grid position of player (agent)\n",
        "    w = findLoc(state, np.array([0,0,1,0])) #find wall\n",
        "    g = findLoc(state, np.array([1,0,0,0])) #find goal\n",
        "    p = findLoc(state, np.array([0,1,0,0])) #find pit\n",
        "    if (not a or not w or not g or not p):\n",
        "        #print('Invalid grid. Rebuilding..')\n",
        "        return initGridPlayer()\n",
        "\n",
        "    return state\n",
        "def dispGrid(state):\n",
        "    grid = np.zeros((4,4), dtype='<U2')\n",
        "    player_loc = findLoc(state, np.array([0,0,0,1]))\n",
        "    wall = findLoc(state, np.array([0,0,1,0]))\n",
        "    goal = findLoc(state, np.array([1,0,0,0]))\n",
        "    pit = findLoc(state, np.array([0,1,0,0]))\n",
        "    for i in range(0,4):\n",
        "        for j in range(0,4):\n",
        "            grid[i,j] = ' '\n",
        "\n",
        "    if player_loc:\n",
        "        grid[player_loc] = 'P' #player\n",
        "    if wall:\n",
        "        grid[wall] = 'W' #wall\n",
        "    if goal:\n",
        "        grid[goal] = '+' #goal\n",
        "    if pit:\n",
        "        grid[pit] = '-' #pit\n",
        "\n",
        "    return grid\n",
        "\n",
        "def makeMove(state, action):\n",
        "    #need to locate player in grid\n",
        "    #need to determine what object (if any) is in the new grid spot the player is moving to\n",
        "    player_loc = findLoc(state, np.array([0,0,0,1]))\n",
        "    wall = findLoc(state, np.array([0,0,1,0]))\n",
        "    goal = findLoc(state, np.array([1,0,0,0]))\n",
        "    pit = findLoc(state, np.array([0,1,0,0]))\n",
        "    state = np.zeros((4,4,4))\n",
        "\n",
        "    #up (row - 1)\n",
        "    if action==0:\n",
        "        new_loc = (player_loc[0] - 1, player_loc[1])\n",
        "        if (new_loc != wall):\n",
        "            if ((np.array(new_loc) <= (3,3)).all() and (np.array(new_loc) >= (0,0)).all()):\n",
        "                state[new_loc][3] = 1\n",
        "    #down (row + 1)\n",
        "    elif action==1:\n",
        "        new_loc = (player_loc[0] + 1, player_loc[1])\n",
        "        if (new_loc != wall):\n",
        "            if ((np.array(new_loc) <= (3,3)).all() and (np.array(new_loc) >= (0,0)).all()):\n",
        "                state[new_loc][3] = 1\n",
        "    #left (column - 1)\n",
        "    elif action==2:\n",
        "        new_loc = (player_loc[0], player_loc[1] - 1)\n",
        "        if (new_loc != wall):\n",
        "            if ((np.array(new_loc) <= (3,3)).all() and (np.array(new_loc) >= (0,0)).all()):\n",
        "                state[new_loc][3] = 1\n",
        "    #right (column + 1)\n",
        "    elif action==3:\n",
        "        new_loc = (player_loc[0], player_loc[1] + 1)\n",
        "        if (new_loc != wall):\n",
        "            if ((np.array(new_loc) <= (3,3)).all() and (np.array(new_loc) >= (0,0)).all()):\n",
        "                state[new_loc][3] = 1\n",
        "\n",
        "    new_player_loc = findLoc(state, np.array([0,0,0,1]))\n",
        "    if (not new_player_loc):\n",
        "        state[player_loc] = np.array([0,0,0,1])\n",
        "    #re-place pit\n",
        "    state[pit][1] = 1\n",
        "    #re-place wall\n",
        "    state[wall][2] = 1\n",
        "    #re-place goal\n",
        "    state[goal][0] = 1\n",
        "\n",
        "    return state\n",
        "def getLoc(state, level):\n",
        "    for i in range(0,4):\n",
        "        for j in range(0,4):\n",
        "            if (state[i,j][level] == 1):\n",
        "                return i,j\n",
        "\n",
        "def getReward(state):\n",
        "    player_loc = getLoc(state, 3)\n",
        "    pit = getLoc(state, 1)\n",
        "    goal = getLoc(state, 0)\n",
        "    if (player_loc == pit):\n",
        "        return -10\n",
        "    elif (player_loc == goal):\n",
        "        return 10\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.optimizers import RMSprop\n",
        "import random\n",
        "model = Sequential()\n",
        "model.add(Dense(164,   input_shape=(64,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(150))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(4))\n",
        "model.add(Activation('linear')) #linear output so we can have range of real-valued outputs\n",
        "rms = RMSprop()\n",
        "model.compile(loss='mse', optimizer=rms)\n",
        "\n",
        "epochs = 3000\n",
        "gamma = 0.975\n",
        "epsilon = 1\n",
        "batchSize = 40\n",
        "buffer = 80\n",
        "replay = []\n",
        "#stores tuples of (S, A, R, S')\n",
        "h = 0\n",
        "state = initGridPlayer()\n",
        "\n",
        "print(state)\n",
        "print('=======================')\n",
        "print(state[1,1])\n",
        "#print(dispGrid(state))\n",
        "\n",
        "# for i in range(epochs):\n",
        "#     state = initGridPlayer() #using the harder state initialization function\n",
        "#     status = 1\n",
        "#     #while game still in progress\n",
        "#     while(status == 1):\n",
        "#         #We are in state S\n",
        "#         #Let's run our Q function on S to get Q values for all possible actions\n",
        "#         qval = model.predict(state.reshape(1,64), batch_size=1)\n",
        "#         if (random.random() < epsilon): #choose random action\n",
        "#             action = np.random.randint(0,4)\n",
        "#         else: #choose best action from Q(s,a) values\n",
        "#             action = (np.argmax(qval))\n",
        "#         #Take action, observe new state S'\n",
        "#         new_state = makeMove(state, action)\n",
        "#         #Observe reward\n",
        "#         reward = getReward(new_state)\n",
        "#         #Experience replay storage\n",
        "#         if (len(replay) < buffer): #if buffer not filled, add to it\n",
        "#             replay.append((state, action, reward, new_state))\n",
        "#         else: #if buffer full, overwrite old values\n",
        "#             if (h < (buffer-1)):\n",
        "#                 h += 1\n",
        "#             else:\n",
        "#                 h = 0\n",
        "#             replay[h] = (state, action, reward, new_state)\n",
        "#             #randomly sample our experience replay memory\n",
        "#             minibatch = random.sample(replay, batchSize)\n",
        "#             X_train = []\n",
        "#             y_train = []\n",
        "#             for memory in minibatch:\n",
        "#                 #Get max_Q(S',a)\n",
        "#                 old_state, action, reward, new_state = memory\n",
        "#                 old_qval = model.predict(old_state.reshape(1,64), batch_size=1)\n",
        "#                 newQ = model.predict(new_state.reshape(1,64), batch_size=1)\n",
        "#                 maxQ = np.max(newQ)\n",
        "#                 y = np.zeros((1,4))\n",
        "#                 y[:] = old_qval[:]\n",
        "#                 if reward == -1: #non-terminal state\n",
        "#                     update = (reward + (gamma * maxQ))\n",
        "#                 else: #terminal state\n",
        "#                     update = reward\n",
        "#                 y[0][action] = update\n",
        "#                 X_train.append(old_state.reshape(64,))\n",
        "#                 y_train.append(y.reshape(4,))\n",
        "#\n",
        "#             X_train = np.array(X_train)\n",
        "#             y_train = np.array(y_train)\n",
        "#             print(\"Game #: %s\" % (i,))\n",
        "#             model.fit(X_train, y_train, batch_size=batchSize,epochs=10  , verbose=1)\n",
        "#             state = new_state\n",
        "#         if reward != -1: #if reached terminal state, update game status\n",
        "#             status = 0\n",
        "#         #clear_output(wait=True)\n",
        "#     if epsilon > 0.1: #decrement epsilon over time\n",
        "#         epsilon -= (1/epochs)\n",
        "\n",
        "\n",
        "def testAlgo(init):\n",
        "    i = 0\n",
        "    if init == 1:\n",
        "        state = initGridPlayer()\n",
        "    print(\"Initial State:\")\n",
        "    print(dispGrid(state))\n",
        "    status = 1\n",
        "    # while game still in progress\n",
        "    while (status == 1):\n",
        "        qval = model.predict(state.reshape(1, 64), batch_size=1)\n",
        "        action = (np.argmax(qval))  # take action with highest Q-value\n",
        "        print('Move #: %s; Taking action: %s' % (i, action))\n",
        "        state = makeMove(state, action)\n",
        "        print(dispGrid(state))\n",
        "        reward = getReward(state)\n",
        "        if reward != -1:\n",
        "            status = 0\n",
        "            print(\"Reward: %s\" % (reward,))\n",
        "        i += 1  # If we're taking more than 10 actions, just stop, we probably can't win this game\n",
        "        if (i > 10):\n",
        "            print(\"Game lost; too many moves.\")\n",
        "            break"
      ],
      "metadata": {
        "id": "S2uhKJkZOHsW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}