{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toanpt74/COLAB_RD/blob/main/ChatBot_Vi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyvi\n",
        "from datetime import datetime\n",
        "import os\n",
        "import sys\n",
        "import chromadb\n",
        "import torch\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    get_peft_model_state_dict,\n",
        "    prepare_model_for_int8_training,\n",
        "    set_peft_model_state_dict,\n",
        "    PeftModel,\n",
        "    PeftConfig,\n",
        "    prepare_model_for_kbit_training\n",
        ")\n",
        "\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from chromadb.utils import embedding_functions\n",
        "\n",
        "os.environ[\"WANDB_MODE\"] = \"offline\"\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "base_model = r'D:\\Tuan\\ChatGPT\\model\\Vistral-7B-Chat'\n",
        "device = \"auto\"\n",
        "model_id = r\"D:\\Tuan\\ChatGPT\\model\\phobert-base-v2\"\n",
        "embedding_fun = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=model_id)\n",
        "\n",
        "\n",
        "def semantic_search(question, num_chunks_distance=5, num_chunks=5):\n",
        "    collection = client.get_or_create_collection('common', metadata={\"hnsw:space\": \"cosine\"},\n",
        "                                                 embedding_function=embedding_fun)\n",
        "    results = collection.query(\n",
        "        query_texts=question,\n",
        "        include=['distances', 'documents', 'metadatas'],\n",
        "        n_results=num_chunks,\n",
        "    )\n",
        "    print(results)\n",
        "    image_path = \"\"\n",
        "    metadatas = results[\"metadatas\"][0]\n",
        "    distance = results['distances'][0][0]\n",
        "    type = metadatas[0][\"type\"]\n",
        "    documents = results[\"documents\"][0]\n",
        "\n",
        "    text = []\n",
        "    if distance < 0.4:\n",
        "        if (type == \"8system\"):\n",
        "            for j in range(len(metadatas)):\n",
        "                if metadatas[j][\"type\"] == \"8system\":\n",
        "                    text.append({\"question\": documents[j], \"anwer\": metadatas[j][\"content\"],\n",
        "                                 \"image_path\": metadatas[j][\"image_path\"]})\n",
        "            # text = metadatas[0][\"content\"]\n",
        "            image_path = metadatas[0][\"image_path\"]\n",
        "            print('------------------------------------')\n",
        "            print(text)\n",
        "            return type, distance, text, image_path\n",
        "    ids = []\n",
        "    index = 0\n",
        "    for cur_chunk_id in results[\"ids\"][0]:\n",
        "        type_question = metadatas[index][\"type\"]\n",
        "        if type_question == 'common':\n",
        "            cur_chunk_id_index = int(cur_chunk_id.split('_')[-1])\n",
        "            head_id = cur_chunk_id[0:len(cur_chunk_id) - len(str(cur_chunk_id_index))]\n",
        "            for i in range(-num_chunks_distance, num_chunks_distance + 1, 1):\n",
        "                if cur_chunk_id_index + i >= 0:\n",
        "                    ids.append(head_id + str(cur_chunk_id_index + i))\n",
        "\n",
        "        index = index + 1\n",
        "    ids = sorted(set(ids))\n",
        "    results = results if len(ids) == 1 else collection.get(ids=ids)\n",
        "    context = \" \".join(s for s in results[\"documents\"])\n",
        "    return type, distance, context, image_path\n",
        "\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True, local_files_only=True)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    device_map=device,\n",
        "    quantization_config=quantization_config,\n",
        "    local_files_only=True\n",
        ")\n",
        "\n",
        "system_prompt = \"Bạn là một trợ lí Tiếng Việt nhiệt tình và trung thực. Hãy luôn trả lời một cách hữu ích nhất có thể, đồng thời giữ an toàn.\\n\"\n",
        "system_prompt += \"Câu trả lời của bạn không nên chứa bất kỳ nội dung gây hại, phân biệt chủng tộc, phân biệt giới tính, độc hại, nguy hiểm hoặc bất hợp pháp nào. Hãy đảm bảo rằng các câu trả lời của bạn không có thiên kiến xã hội và mang tính tích cực.\"\n",
        "system_prompt += \"Nếu một câu hỏi không có ý nghĩa hoặc không hợp lý về mặt thông tin, hãy giải thích tại sao thay vì trả lời một điều gì đó không chính xác. Nếu bạn không biết câu trả lời cho một câu hỏi, hãy trẳ lời là bạn không biết và vui lòng không chia sẻ thông tin sai lệch.\\n\"\n",
        "\n",
        "client = chromadb.PersistentClient('QAVectorDB_Vi')\n",
        "collection = client.get_or_create_collection('common', metadata={\"hnsw:space\": \"cosine\"},\n",
        "                                             embedding_function=embedding_fun)\n",
        "\n",
        "while True:\n",
        "    question = input(\"Nhap cau hoi:\")\n",
        "    if question == \"\":\n",
        "        break\n",
        "    type, distance, context, image_path = semantic_search(question, num_chunks_distance=1)\n",
        "\n",
        "    if type == \"common\":\n",
        "        print(context)\n",
        "        system_prompt += context\n",
        "        conversation = [{\"role\": \"system\", \"content\": system_prompt}]\n",
        "        conversation.append({\"role\": \"user\", \"content\": f\"{question}\"})\n",
        "\n",
        "        input_ids = tokenizer.apply_chat_template(conversation, return_tensors=\"pt\").to(model.device)\n",
        "        print(input_ids[0].shape)\n",
        "\n",
        "        out_ids = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_new_tokens=1000,\n",
        "            do_sample=True,\n",
        "            top_p=0.95,\n",
        "            top_k=40,\n",
        "            temperature=0.1,\n",
        "            repetition_penalty=1.05,\n",
        "        )\n",
        "        assistant = tokenizer.batch_decode(out_ids[:, input_ids.size(1):], skip_special_tokens=True)[0].strip()\n",
        "        print(\"Assistant: \", assistant)\n",
        "        conversation.append({\"role\": \"assistant\", \"content\": assistant})\n",
        "    elif (type == \"8system\"):\n",
        "        print(context)\n",
        "        print(image_path)\n",
        "\n",
        "    # inputs_not_chat = tokenizer.encode_plus(\"[INST] Tell me about fantasy football? [/INST]\", return_tensors=\"pt\")['input_ids'].to('cuda')\n",
        "    #\n",
        "    # generated_ids = model.generate(inputs_not_chat,\n",
        "    #                                max_new_tokens=1000,\n",
        "    #                                do_sample=True)\n",
        "    # decoded = tokenizer.batch_decode(generated_ids)\n"
      ],
      "metadata": {
        "id": "JKHwOvJxl0Lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t4RckuC1l0b-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y3L8d3zMl0fT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rn97yjzLl0ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Vp7SXWgl0l1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yyNm0RXWl0o-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KcRyqd1Zl0sc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}