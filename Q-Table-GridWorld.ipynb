{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toanpt74/COLAB_RD/blob/main/Q-Table-GridWorld.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "class Environment():\n",
        "    def __init__(self, grid):\n",
        "        self.grid = grid\n",
        "        self.n_rows = len(grid)\n",
        "        self.n_cols = len(grid[0])\n",
        "        self.positions = self._positions()\n",
        "        self.starting_positions = [p for p in self.positions\n",
        "                                   if not self.is_terminal_state(p)]\n",
        "\n",
        "    def actions(self, pos):\n",
        "        \"\"\"possible actions for a state (position)\"\"\"\n",
        "        r, c = pos\n",
        "        actions = []\n",
        "        if r > 0:\n",
        "            actions.append('up')\n",
        "        if r < self.n_rows - 1:\n",
        "            actions.append('down')\n",
        "        if c > 0:\n",
        "            actions.append('left')\n",
        "        if c < self.n_cols - 1:\n",
        "            actions.append('right')\n",
        "        return actions\n",
        "\n",
        "    def value(self, pos):\n",
        "        \"\"\"retrieve the reward value for a position\"\"\"\n",
        "        r, c = pos\n",
        "        return self.grid[r][c]\n",
        "\n",
        "    def _positions(self):\n",
        "        \"\"\"all positions\"\"\"\n",
        "        positions = []\n",
        "        for r, row in enumerate(self.grid):\n",
        "            for c, _ in enumerate(row):\n",
        "                positions.append((r, c))\n",
        "        return positions\n",
        "\n",
        "    def is_terminal_state(self, state):\n",
        "        \"\"\"tell us if the state ends the game\"\"\"\n",
        "        val = self.value(state)\n",
        "\n",
        "        return val == 10 or val == -10\n",
        "\n",
        "    def reward(self, state):\n",
        "        \"\"\"the reward of a state:\n",
        "        -1 if it's a hole,\n",
        "        -1 if it's an empty space (to penalize each move),\n",
        "        otherwise, the value of the state\"\"\"\n",
        "        val = self.value(state)\n",
        "        # if val is None or val == 0:\n",
        "        #     return -1\n",
        "        return val\n",
        "\n",
        "class QLearner():\n",
        "    def __init__(self, state, environment, rewards, discount=0.5, explore=0.5, learning_rate=1):\n",
        "        \"\"\"\n",
        "        - state: the agent's starting state\n",
        "        - rewards: a reward function, taking a state as input, or a mapping of states to a reward value\n",
        "        - discount: how much the agent values future rewards over immediate rewards\n",
        "        - explore: with what probability the agent \"explores\", i.e. chooses a random action\n",
        "        - learning_rate: how quickly the agent learns. For deterministic environments (like ours), this should be left at 1\n",
        "        \"\"\"\n",
        "        self.discount = discount\n",
        "        self.explore = explore\n",
        "        self.learning_rate = learning_rate\n",
        "        self.R = rewards.get if isinstance(rewards, dict) else rewards\n",
        "\n",
        "        # our state is just our position\n",
        "        self.state = state\n",
        "        self.reward = 0\n",
        "        self.env = environment\n",
        "\n",
        "        # initialize Q\n",
        "        self.Q = {}\n",
        "\n",
        "        self.q_table = np.zeros((6,4))\n",
        "    def reset(self, state):\n",
        "        self.state = state\n",
        "        self.reward = 0\n",
        "\n",
        "    def actions(self, state):\n",
        "        return self.env.actions(state)\n",
        "\n",
        "    def _take_action(self, state, action):\n",
        "        r, c = state\n",
        "        if action == 'up':\n",
        "            r -= 1\n",
        "        elif action == 'down':\n",
        "            r += 1\n",
        "        elif action == 'right':\n",
        "            c += 1\n",
        "        elif action == 'left':\n",
        "            c -= 1\n",
        "\n",
        "        # return new state\n",
        "        return (r, c)\n",
        "\n",
        "    def step(self, action=None):\n",
        "        \"\"\"take an action\"\"\"\n",
        "        # check possible actions given state\n",
        "        actions = self.actions(self.state)\n",
        "\n",
        "        # if this is the first time in this state,\n",
        "        # initialize possible actions\n",
        "        if self.state not in self.Q:\n",
        "            self.Q[self.state] = {a: 0 for a in actions}\n",
        "\n",
        "        if action is None:\n",
        "            if random.random() < self.explore:\n",
        "                action = random.choice(actions)\n",
        "            else:\n",
        "                action = self._best_action(self.state)\n",
        "        elif action not in actions:\n",
        "            raise ValueError('unrecognized action!')\n",
        "\n",
        "        # remember this state and action\n",
        "        # so we can later remember\n",
        "        # \"from this state, taking this action is this valuable\"\n",
        "        prev_state = self.state\n",
        "\n",
        "        # update state\n",
        "        self.state = self._take_action(self.state, action)\n",
        "\n",
        "        # update the previous state/action based on what we've learned\n",
        "        self._learn(prev_state, action, self.state)\n",
        "        return action\n",
        "\n",
        "    def _best_action(self, state):\n",
        "        \"\"\"choose the best action given a state\"\"\"\n",
        "        actions_rewards = list(self.Q[state].items())\n",
        "        return max(actions_rewards, key=lambda x: x[1])[0]\n",
        "\n",
        "    def _learn(self, prev_state, action, new_state):\n",
        "        \"\"\"update Q-value for the last taken action\"\"\"\n",
        "        moves = {\"left\": 0, \"right\": 1, \"up\": 2, \"down\": 3}\n",
        "        if new_state not in self.Q:\n",
        "            self.Q[new_state] = {a: 0 for a in self.actions(new_state)}\n",
        "        reward = self.R(new_state)\n",
        "        self.reward += reward\n",
        "        c = self.Q[new_state].values()\n",
        "        b = max(self.Q[new_state].values())\n",
        "        a = self.Q[prev_state][action] + self.learning_rate * (\n",
        "                    reward + self.discount * max(self.Q[new_state].values()) - self.Q[prev_state][action])\n",
        "        self.Q[prev_state][action] = a\n",
        "        row = prev_state[0]\n",
        "        col = prev_state[1]\n",
        "        index = row * 3  + col\n",
        "        q_col = moves[action]\n",
        "        self.q_table[index,q_col] = a\n",
        "\n",
        "env = Environment([\n",
        "    [0, 1, 0],\n",
        "    [0, -10, 10]\n",
        "])\n",
        "\n",
        "pos = (0,0)\n",
        "agent = QLearner(pos, env, env.reward, discount=0.9, learning_rate=0.1)\n",
        "agent.step(action='right')\n",
        "print(agent.Q)\n",
        "agent.step(action='down')\n",
        "print(agent.Q)\n",
        "# agent.step(action='down')\n",
        "# print(agent.Q)\n",
        "agent.step(action='right')\n",
        "print(agent.Q)\n",
        "print(agent.q_table)"
      ],
      "metadata": {
        "id": "S2uhKJkZOHsW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}