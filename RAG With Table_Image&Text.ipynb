{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toanpt74/COLAB_RD/blob/main/RAG%20With%20Table_Image%26Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "eEukNkjRvcMO",
        "outputId": "fc6ce3b1-c2cc-4e33-dcb8-307c757e5ad0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk==3.9.1"
      ],
      "metadata": {
        "id": "w-kZ3cWlvcP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install langchain langchain-chroma \"unstructured[all-docs]\" pydantic lxml langchainhub"
      ],
      "metadata": {
        "id": "C-8JuIjsvcTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community"
      ],
      "metadata": {
        "id": "TUleH-74vcW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install poppler-utils\n",
        "!sudo apt install tesseract-ocr"
      ],
      "metadata": {
        "id": "GozBAwFzvcan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "https://github.com/KxSystems/kdbai-samples/blob/main/multimodal_RAG_unified_text/multi_modal_demo.ipynb\n",
        "https://www.getty.edu/publications/resources/virtuallibrary/0892360224.pdf\n",
        "https://tam159.medium.com/9-methods-to-enhance-the-performance-of-a-llm-rag-application-3bedfdc842e1\n",
        "https://medium.com/@kbouziane.ai/harnessing-rag-for-text-tables-and-images-a-comprehensive-guide-ca4d2d420219\n",
        "https://github.com/langchain-ai/langchain/blob/master/cookbook/multi_modal_RAG_chroma.ipynb\n",
        "from huggingface_hub import login\n",
        "access_token_read = \"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\"\n",
        "\n",
        "#\"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\"\n",
        "\n",
        "login(token = access_token_read)\n",
        "\n",
        "!pip install nltk==3.9.1\n",
        "! pip install langchain langchain-chroma \"unstructured[all-docs]\" pydantic lxml langchainhub\n",
        "!apt-get install poppler-utils\n",
        "!sudo apt install tesseract-ocr\n",
        "!pip install langchain_community\n",
        "!wget https://arxiv.org/pdf/2304.08485.pdf\n",
        "\n",
        "path='/content/'\n",
        "from typing import Any\n",
        "\n",
        "from pydantic import BaseModel\n",
        "from unstructured.partition.pdf import partition_pdf\n",
        "\n",
        "# Get elements\n",
        "raw_pdf_elements = partition_pdf(\n",
        "    filename=path + \"2304.08485.pdf\",\n",
        "    # Using pdf format to find embedded image blocks\n",
        "    extract_images_in_pdf=True,\n",
        "    # Use layout model (YOLOX) to get bounding boxes (for tables) and find titles\n",
        "    # Titles are any sub-section of the document\n",
        "    infer_table_structure=True,\n",
        "    # Post processing to aggregate text once we have the title\n",
        "    chunking_strategy=\"by_title\",\n",
        "    # Chunking params to aggregate text blocks\n",
        "    # Attempt to create a new chunk 3800 chars\n",
        "    # Attempt to keep chunks > 2000 chars\n",
        "    # Hard max on chunks\n",
        "    max_characters=4000,\n",
        "    new_after_n_chars=3800,\n",
        "    combine_text_under_n_chars=2000,\n",
        "    image_output_dir_path=path,\n",
        ")\n",
        "# Create a dictionary to store counts of each type\n",
        "category_counts = {}\n",
        "\n",
        "for element in raw_pdf_elements:\n",
        "    category = str(type(element))\n",
        "    if category in category_counts:\n",
        "        category_counts[category] += 1\n",
        "    else:\n",
        "        category_counts[category] = 1\n",
        "\n",
        "# Unique_categories will have unique elements\n",
        "unique_categories = set(category_counts.keys())\n",
        "category_counts\n",
        "class Element(BaseModel):\n",
        "    type: str\n",
        "    text: Any\n",
        "\n",
        "\n",
        "# Categorize by type\n",
        "categorized_elements = []\n",
        "for element in raw_pdf_elements:\n",
        "    if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
        "        categorized_elements.append(Element(type=\"table\", text=str(element)))\n",
        "    elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
        "        categorized_elements.append(Element(type=\"text\", text=str(element)))\n",
        "\n",
        "# Tables\n",
        "table_elements = [e for e in categorized_elements if e.type == \"table\"]\n",
        "print(len(table_elements))\n",
        "\n",
        "# Text\n",
        "text_elements = [e for e in categorized_elements if e.type == \"text\"]\n",
        "print(len(text_elements))\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, Pipeline\n",
        "import torch\n",
        "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "model = HuggingFacePipeline.from_model_id(\n",
        "    model_id ='mistralai/Mistral-7B-Instruct-v0.2',\n",
        "    task =\"text-generation\",\n",
        "    device=0,\n",
        "    batch_size=2,\n",
        "    model_kwargs={\"temperature\":0, \"torch_dtype\":torch.bfloat16},\n",
        "    pipeline_kwargs={\n",
        "          #\"return_full_text\":True,\n",
        "          \"temperature\":0.5,\n",
        "          \"max_new_tokens\":1000,\n",
        "          \"repetition_penalty\":1.1,\n",
        "          #\"return_dict_in_generate\":True,\n",
        "          },\n",
        ")\n",
        "\n",
        "# Prompt\n",
        "prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text. Give a concise summary of the table or text. Table or text chunk: {element} \"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
        "\n",
        "# Summary chain\n",
        "#model = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
        "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
        "\n",
        "# Apply to tables\n",
        "tables = [i.text for i in table_elements]\n",
        "table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "//////////////////////////////////////////////\n",
        "TAO TIEU DE CHO IMAGE\n",
        "////////////////////////////////////////////////////\n",
        "import requests\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
        "\n",
        "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
        "model = LlavaForConditionalGeneration.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        ").to(0)\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "# Define a chat histiry and use `apply_chat_template` to get correctly formatted prompt\n",
        "# Each value in \"content\" has to be a list of dicts with types (\"text\", \"image\")\n",
        "conversation = [\n",
        "    {\n",
        "\n",
        "      \"role\": \"user\",\n",
        "      \"content\": [\n",
        "          {\"type\": \"text\", \"text\": \"What are these?\"},\n",
        "          {\"type\": \"image\"},\n",
        "        ],\n",
        "    },\n",
        "]\n",
        "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
        "\n",
        "image_file = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "raw_image = Image.open(requests.get(image_file, stream=True).raw)\n",
        "inputs = processor(images=raw_image, text=prompt, return_tensors='pt').to(0, torch.float16)\n",
        "\n",
        "output = model.generate(**inputs, max_new_tokens=200, do_sample=False)\n",
        "print(processor.decode(output[0][2:], skip_special_tokens=True))\n",
        "\n",
        "///============================================================\n",
        "\n",
        "!pip install langchain-experimental\n",
        "!pip install open-clip-torch\n",
        "!wget https://www.loc.gov/lcm/pdf/LCM_2020_1112.pdf\n",
        "path='/content/'\n",
        "from unstructured.partition.pdf import partition_pdf\n",
        "\n",
        "raw_pdf_elements = partition_pdf(\n",
        "    filename='/content/LCM_2020_1112.pdf',\n",
        "    extract_images_in_pdf=True,\n",
        "    infer_table_structure=True,\n",
        "    chunking_strategy=\"by_title\",\n",
        "    max_characters=4000,\n",
        "    new_after_n_chars=3800,\n",
        "    combine_text_under_n_chars=2000,\n",
        "    image_output_dir_path=path,\n",
        ")\n",
        "\n",
        "# Categorize text elements by type\n",
        "tables = []\n",
        "texts = []\n",
        "for element in raw_pdf_elements:\n",
        "    if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
        "        tables.append(str(element))\n",
        "    elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
        "        texts.append(str(element))\n",
        "\n",
        "path='/content/figures'\n",
        "import os\n",
        "import uuid\n",
        "\n",
        "import chromadb\n",
        "import numpy as np\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_experimental.open_clip import OpenCLIPEmbeddings\n",
        "from PIL import Image as _PILImage\n",
        "\n",
        "# Get image URIs with .jpg extension only\n",
        "image_uris = sorted(\n",
        "    [\n",
        "        os.path.join(path, image_name)\n",
        "        for image_name in os.listdir(path)\n",
        "        if image_name.endswith(\".jpg\")\n",
        "    ]\n",
        ")\n",
        "# Create chroma\n",
        "vectorstore = Chroma(\n",
        "    collection_name=\"mm_rag_clip_photos\", embedding_function=OpenCLIPEmbeddings()\n",
        ")\n",
        "# Add images\n",
        "vectorstore.add_images(uris=image_uris)\n",
        "# Add documents\n",
        "vectorstore.add_texts(texts=texts)\n",
        "# Make retriever\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "import base64\n",
        "import io\n",
        "from io import BytesIO\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def resize_base64_image(base64_string, size=(128, 128)):\n",
        "    \"\"\"\n",
        "    Resize an image encoded as a Base64 string.\n",
        "\n",
        "    Args:\n",
        "    base64_string (str): Base64 string of the original image.\n",
        "    size (tuple): Desired size of the image as (width, height).\n",
        "\n",
        "    Returns:\n",
        "    str: Base64 string of the resized image.\n",
        "    \"\"\"\n",
        "    # Decode the Base64 string\n",
        "    img_data = base64.b64decode(base64_string)\n",
        "    img = Image.open(io.BytesIO(img_data))\n",
        "\n",
        "    # Resize the image\n",
        "    resized_img = img.resize(size, Image.LANCZOS)\n",
        "\n",
        "    # Save the resized image to a bytes buffer\n",
        "    buffered = io.BytesIO()\n",
        "    resized_img.save(buffered, format=img.format)\n",
        "\n",
        "    # Encode the resized image to Base64\n",
        "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
        "\n",
        "\n",
        "def is_base64(s):\n",
        "    \"\"\"Check if a string is Base64 encoded\"\"\"\n",
        "    try:\n",
        "        return base64.b64encode(base64.b64decode(s)) == s.encode()\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "\n",
        "def split_image_text_types(docs):\n",
        "    \"\"\"Split numpy array images and texts\"\"\"\n",
        "    images = []\n",
        "    text = []\n",
        "    for doc in docs:\n",
        "        doc = doc.page_content  # Extract Document contents\n",
        "        if is_base64(doc):\n",
        "            # Resize image to avoid OAI server error\n",
        "            images.append(\n",
        "                resize_base64_image(doc, size=(250, 250))\n",
        "            )  # base64 encoded str\n",
        "        else:\n",
        "            text.append(doc)\n",
        "    return {\"images\": images, \"texts\": text}\n",
        "\n",
        "from operator import itemgetter\n",
        "\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "def prompt_func(data_dict):\n",
        "    # Joining the context texts into a single string\n",
        "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
        "    messages = []\n",
        "\n",
        "    # Adding image(s) to the messages if present\n",
        "    if data_dict[\"context\"][\"images\"]:\n",
        "        image_message = {\n",
        "            \"type\": \"image_url\",\n",
        "            \"image_url\": {\n",
        "                \"url\": f\"data:image/jpeg;base64,{data_dict['context']['images'][0]}\"\n",
        "            },\n",
        "        }\n",
        "        messages.append(image_message)\n",
        "\n",
        "    # Adding the text message for analysis\n",
        "    text_message = {\n",
        "        \"type\": \"text\",\n",
        "        \"text\": (\n",
        "            \"As an expert art critic and historian, your task is to analyze and interpret images, \"\n",
        "            \"considering their historical and cultural significance. Alongside the images, you will be \"\n",
        "            \"provided with related text to offer context. Both will be retrieved from a vectorstore based \"\n",
        "            \"on user-input keywords. Please use your extensive knowledge and analytical skills to provide a \"\n",
        "            \"comprehensive summary that includes:\\n\"\n",
        "            \"- A detailed description of the visual elements in the image.\\n\"\n",
        "            \"- The historical and cultural context of the image.\\n\"\n",
        "            \"- An interpretation of the image's symbolism and meaning.\\n\"\n",
        "            \"- Connections between the image and the related text.\\n\\n\"\n",
        "            f\"User-provided keywords: {data_dict['question']}\\n\\n\"\n",
        "            \"Text and / or tables:\\n\"\n",
        "            f\"{formatted_texts}\"\n",
        "        ),\n",
        "    }\n",
        "    messages.append(text_message)\n",
        "\n",
        "    return [HumanMessage(content=messages)]\n",
        "\n",
        "docs = retriever.invoke(\"Woman with children\", k=1)\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "\n",
        "def plt_img_base64(img_base64):\n",
        "    # Create an HTML img tag with the base64 string as the source\n",
        "    image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'\n",
        "\n",
        "    # Display the image by rendering the HTML\n",
        "    display(HTML(image_html))\n",
        "for doc in docs:\n",
        "  if is_base64(doc.page_content):\n",
        "        plt_img_base64(doc.page_content)\n",
        "        #print(doc.page_content)\n",
        "  print('-------------------------')\n",
        ""
      ],
      "metadata": {
        "id": "pqaJdfeiwFnx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}