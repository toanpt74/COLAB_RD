{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toanpt74/COLAB_RD/blob/main/VAE_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Lambda, Input, Dense\n",
        "from keras.models import Model\n",
        "from keras.datasets import mnist\n",
        "from keras.losses import mse, binary_crossentropy\n",
        "from keras import backend as K\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from keras.layers import Input, Conv2D, Conv2DTranspose, Flatten, Dense, Reshape, ReLU, Lambda, BatchNormalization\n",
        "import os\n",
        "import keras\n",
        "from keras import layers\n",
        "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "def preprocess(array):\n",
        "    \"\"\"\n",
        "    Normalizes the supplied array and reshapes it into the appropriate format.\n",
        "    \"\"\"\n",
        "    array = array.astype(\"float32\") / 255.0\n",
        "    array = np.reshape(array, (len(array), 28, 28, 1))\n",
        "    return array\n",
        "\n",
        "def noise(array):\n",
        "    \"\"\"\n",
        "    Adds random noise to each image in the supplied array.\n",
        "    \"\"\"\n",
        "    noise_factor = 0.4\n",
        "    noisy_array = array + noise_factor * np.random.normal(\n",
        "        loc=0.0, scale=1.0, size=array.shape\n",
        "    )\n",
        "    return np.clip(noisy_array, 0.0, 1.0)\n",
        "\n",
        "def display(array1, array2):\n",
        "    \"\"\"\n",
        "    Displays ten random images from each one of the supplied arrays.\n",
        "    \"\"\"\n",
        "    n = 10\n",
        "    indices = np.random.randint(len(array1), size=n)\n",
        "    images1 = array1[indices, :]\n",
        "    images2 = array2[indices, :]\n",
        "    plt.figure(figsize=(20, 4))\n",
        "    for i, (image1, image2) in enumerate(zip(images1, images2)):\n",
        "        ax = plt.subplot(2, n, i + 1)\n",
        "        plt.imshow(image1.reshape(28, 28))\n",
        "        plt.gray()\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "\n",
        "        ax = plt.subplot(2, n, i + 1 + n)\n",
        "        plt.imshow(image2.reshape(28, 28))\n",
        "        plt.gray()\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "    plt.show()\n",
        "\n",
        "(train_data, _), (test_data, _) = mnist.load_data()\n",
        "\n",
        "# Normalize and reshape the data\n",
        "train_data = preprocess(train_data)\n",
        "test_data = preprocess(test_data)\n",
        "# Create a copy of the data with added noise\n",
        "noisy_train_data = noise(train_data)\n",
        "noisy_test_data = noise(test_data)\n",
        "# Display the train data and a version of it with added noise\n",
        "#display(train_data, noisy_train_data)\n",
        "\n",
        "ROW = 28\n",
        "COL = 28\n",
        "def pre_image(image_filename):\n",
        "    print(\"Load file:\" + image_filename)\n",
        "    img_raw = tf.io.read_file(image_filename)\n",
        "    image = tf.image.decode_bmp(img_raw)\n",
        "    image = tf.cast(image, dtype=tf.float32)\n",
        "    image = tf.image.resize(image, (ROW, COL))\n",
        "    image = image / 255.0\n",
        "    image = tf.reshape(image, shape=(ROW, COL, 1,)) #shape(hang, cot)\n",
        "    print(image.shape)\n",
        "    return image\n",
        "\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "original_dim = 28*28\n",
        "input_shape = (28,28,1)\n",
        "intermediate_dim = 128\n",
        "batch_size = 128\n",
        "latent_dim = 32\n",
        "epochs = 200\n",
        "def display(array1, array2):\n",
        "    \"\"\"\n",
        "    Displays ten random images from each one of the supplied arrays.\n",
        "    \"\"\"\n",
        "    n = 10\n",
        "    indices = np.random.randint(len(array1), size=n)\n",
        "    images1 = array1[indices, :]\n",
        "    images2 = array2[indices, :]\n",
        "    plt.figure(figsize=(20, 4))\n",
        "    for i, (image1, image2) in enumerate(zip(images1, images2)):\n",
        "        ax = plt.subplot(2, n, i + 1)\n",
        "        plt.imshow(image1.reshape(28, 28))\n",
        "        plt.gray()\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "\n",
        "        ax = plt.subplot(2, n, i + 1 + n)\n",
        "        plt.imshow(image2.reshape(28, 28))\n",
        "        plt.gray()\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "    plt.show()\n",
        "class Sampling(layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        mu, sigma = inputs\n",
        "        batch = tf.shape(mu)[0]\n",
        "        dim = tf.shape(mu)[1]\n",
        "        epsilon = keras.backend.random_normal(shape=(batch, dim))\n",
        "        z = mu + tf.exp(0.5 * sigma) * epsilon\n",
        "        return z\n",
        "\n",
        "\n",
        "def encoder_layer(inputs, latent_dim):\n",
        "    # Block-1\n",
        "    x = Conv2D(filters=32, kernel_size=3, strides=1, padding='same', activation='relu', name='conv_1')(inputs)\n",
        "    x = BatchNormalization(name='bn_1')(x)\n",
        "\n",
        "    # Block-2\n",
        "    x = Conv2D(64, kernel_size=3, strides=2, padding='same', activation='relu', name='conv_2')(x)\n",
        "    x = BatchNormalization(name='bn_2')(x)\n",
        "    # Block-3\n",
        "    x = Conv2D(64, kernel_size=3, strides=2, padding='same', activation='relu', name='conv_3')(x)\n",
        "    x = BatchNormalization(name='bn_3')(x)\n",
        "\n",
        "    # Block-4\n",
        "    x = Conv2D(filters=64, kernel_size=3, strides=1, padding='same', activation='relu', name='conv_4')(x)\n",
        "    x = BatchNormalization(name='bn_4')(x)\n",
        "    batch_3 = x #BatchNormalization()(x)\n",
        "    #Final Block\n",
        "    x = Flatten()(batch_3)\n",
        "    mu = layers.Dense(latent_dim, name='latent_mu')(x)\n",
        "    sigma = layers.Dense(latent_dim, name='latent_sigma')(x)\n",
        "    return mu, sigma, batch_3.shape\n",
        "\n",
        "def encoder_model(latent_dim, input_shape):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    mu, sigma, conv_shape = encoder_layer(inputs, latent_dim=latent_dim)\n",
        "    z = Sampling()((mu, sigma))\n",
        "    model = Model(inputs, outputs=[mu, sigma, z], name='Encoder')\n",
        "    model.summary()\n",
        "    return model, conv_shape\n",
        "\n",
        "def decoder_layer(inputs, conv_shape):\n",
        "    units = conv_shape[1] * conv_shape[2] * conv_shape[3]\n",
        "    x = Dense(units, name='dense_1')(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Reshape((conv_shape[1], conv_shape[2], conv_shape[3]))(x)\n",
        "    # Block-1\n",
        "    x = Conv2DTranspose(filters=64, kernel_size=3, strides=1, padding='same', activation='relu',name='conv_transpose_1')(x)\n",
        "    x = BatchNormalization(name='bn_1')(x)\n",
        "    # Block-2\n",
        "    x = Conv2DTranspose(filters=64, kernel_size=3, strides=2, padding='same', activation='relu',name='conv_transpose_2')(x)\n",
        "    x = BatchNormalization(name='bn_2')(x)\n",
        "\n",
        "    # Block-3\n",
        "    x = Conv2DTranspose(filters=64, kernel_size=3, strides=2, padding='same', activation='relu',\n",
        "                               name='conv_transpose_3')(x)\n",
        "    x = BatchNormalization(name='bn_3')(x)\n",
        "    # Block-4\n",
        "    x = Conv2DTranspose(filters=32, kernel_size=3, strides=1, padding='same', activation='relu',\n",
        "                               name='conv_transpose_4')(x)\n",
        "    x = BatchNormalization(name='bn_4')(x)\n",
        "    outputs = Conv2DTranspose(filters=1, kernel_size=3, strides=1, padding='same', activation='sigmoid',name='out')(x)\n",
        "    return outputs\n",
        "\n",
        "def decoder_model(latent_dim, conv_shape):\n",
        "    inputs = Input(shape=(latent_dim,))\n",
        "    outputs = decoder_layer(inputs, conv_shape)\n",
        "    model = Model(inputs, outputs, name='Decoder')\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "def kl_reconstruction_loss(inputs, outputs, mu, sigma):\n",
        "    kl_loss = 1 + sigma - tf.square(mu) - tf.math.exp(sigma)\n",
        "    return tf.reduce_mean(kl_loss) * -0.5\n",
        "\n",
        "\n",
        "def vae_model(encoder, decoder, input_shape):\n",
        "    inputs = keras.layers.Input(shape=input_shape)\n",
        "    mu, sigma, z = encoder(inputs)\n",
        "    reconstructed = decoder(z)\n",
        "    model = keras.Model(inputs=inputs, outputs=reconstructed)\n",
        "    loss = kl_reconstruction_loss(inputs, z, mu, sigma)\n",
        "    model.add_loss(loss)\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_models(input_shape, latent_dim):\n",
        "    encoder, conv_shape = encoder_model(latent_dim=latent_dim, input_shape=input_shape)\n",
        "    decoder = decoder_model(latent_dim=latent_dim, conv_shape=conv_shape)\n",
        "    vae = vae_model(encoder, decoder, input_shape=input_shape)\n",
        "    return encoder, decoder, vae\n",
        "\n",
        "encoder, decoder, vae = get_models(input_shape=input_shape, latent_dim=latent_dim)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
        "loss_metric = keras.metrics.Mean()\n",
        "mse_loss = keras.losses.MeanSquaredError()\n",
        "\n",
        "x_train =  tf.data.Dataset.from_tensor_slices((train_data))\n",
        "\n",
        "train_ds = (x_train\n",
        "            .shuffle(len(x_train))\n",
        "                # .map(pre_image, num_parallel_calls=AUTO)\n",
        "                .batch(batch_size)\n",
        "                .prefetch(buffer_size=AUTO))\n",
        "\n",
        "model_path =r'E:\\PROJECT-AI\\ImageClassification\\models'\n",
        "\n",
        "# for epoch in range(epochs):\n",
        "#     print('Start of epoch %d' % (epoch,))\n",
        "#     for step, x_batch_train in enumerate(train_ds):\n",
        "#         with tf.GradientTape() as tape:\n",
        "#             reconstructed = vae(x_batch_train)\n",
        "#             flattened_inputs = tf.reshape(x_batch_train, shape=[-1])\n",
        "#             flattened_outputs = tf.reshape(reconstructed, shape=[-1])\n",
        "#             loss = mse_loss(flattened_inputs, flattened_outputs) * COL * ROW\n",
        "#             loss += sum(vae.losses)\n",
        "#\n",
        "#         grads = tape.gradient(loss, vae.trainable_weights)\n",
        "#         optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
        "#         loss_metric(loss)\n",
        "#         print('Epoch: %s step: %s mean loss = %s' % (epoch, step, loss_metric.result().numpy()))\n",
        "#     if epoch % 5 == 0:\n",
        "#         vae.save(f'VAE_MNIST\\\\model_{epoch}__{loss}', save_format=\"tf\")\n",
        "\n",
        "model = tf.keras.models.load_model(r'E:\\ToanPT\\1.Code_train_Unet\\VAE_MNIST\\model_195__4.01559591293335')\n",
        "\n",
        "re = model.predict(x_train)\n",
        "\n",
        "display(x_train,re)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JKHwOvJxl0Lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t4RckuC1l0b-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y3L8d3zMl0fT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rn97yjzLl0ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Vp7SXWgl0l1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yyNm0RXWl0o-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KcRyqd1Zl0sc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}