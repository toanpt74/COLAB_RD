{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toanpt74/COLAB_RD/blob/main/Abnormal_Detect_Time_Serries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, LSTM, RepeatVector, LeakyReLU\n",
        "from keras import regularizers\n",
        "from keras.layers import Flatten, Dense, Dropout, Lambda, TimeDistributed\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from numpy import random\n",
        "\n",
        "# df_data = pd.read_csv(\n",
        "#         r'D:\\Private_Documents\\111\\data.csv', parse_dates=True\n",
        "#     )\n",
        "\n",
        "\n",
        "def get_data(timesteps, input_dim, path):\n",
        "    # read data from file\n",
        "    # data = np.fromfile(r'D:\\Private_Documents\\111\\data.csv').reshape(419,13)\n",
        "    df_data = pd.read_csv(\n",
        "        path, parse_dates=True\n",
        "    )\n",
        "    #df_data['value'] = df_data['value'].astype(np.float64)\n",
        "    #scaler = StandardScaler()\n",
        "    #df_data['value'] = scaler.fit_transform(df_data['value'].values.reshape(-1, 1))\n",
        "\n",
        "    data = np.array(df_data['value'])\n",
        "    L = int(len(data) / input_dim)\n",
        "    data = data[0:L * input_dim]\n",
        "    data = data.reshape(-1, input_dim)\n",
        "    dataX = []\n",
        "\n",
        "    for i in range(len(data)):\n",
        "        x = data[i:(i + timesteps), :]\n",
        "        dataX.append(x)\n",
        "    return np.array(dataX)\n",
        "\n",
        "\n",
        "timesteps = 1\n",
        "input_dim = 30\n",
        "batch_size = 32\n",
        "latent_dim = 100\n",
        "EPOCHS =1000\n",
        "path_data_train = r'D:\\data.csv'\n",
        "path_data_test = r'D:\\test.csv'\n",
        "\n",
        "data_train = get_data(timesteps, input_dim, path_data_train)\n",
        "data_test = get_data(timesteps, input_dim, path_data_test)\n",
        "\n",
        "# data_train = np.expand_dims(data_train, axis=1)\n",
        "# data_test = np.expand_dims(data_test, axis=1)\n",
        "\n",
        "print(data_train.shape)\n",
        "print(data_test.shape)\n",
        "print(data_test)\n",
        "minTrain = np.min(data_train)\n",
        "maxTrain = np.max(data_train)\n",
        "\n",
        "minTest = np.min(data_test)\n",
        "maxTest = np.max(data_test)\n",
        "\n",
        "if minTrain < minTest:\n",
        "    min = minTrain\n",
        "else:\n",
        "    min = minTest\n",
        "if maxTrain > maxTest:\n",
        "    max = maxTrain\n",
        "else:\n",
        "    max = maxTest\n",
        "\n",
        "\n",
        "\n",
        "x_test = (data_test - min) / (max - min)\n",
        "x_train = (data_train - min) / (max - min)\n",
        "\n",
        "x_train_tensor = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
        "\n",
        "\n",
        "def autoencoder_model1(X):\n",
        "    inputs = Input(shape=(X.shape[1], X.shape[2]))\n",
        "    L1 = LSTM(2*latent_dim, activation='relu', return_sequences=True)(inputs)\n",
        "    L2 = LSTM(latent_dim, activation='relu', return_sequences=False)(L1)\n",
        "    L3 = RepeatVector(X.shape[1])(L2)\n",
        "    L4 = LSTM(latent_dim, activation='relu', return_sequences=True)(L3)\n",
        "    L5 = LSTM(2*latent_dim, activation='relu', return_sequences=True)(L4)\n",
        "    output = TimeDistributed(Dense(X.shape[2]))(L5)\n",
        "    model = Model(inputs=inputs, outputs=output)\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def autoencoder_model(X):\n",
        "    inputs = Input(shape=(X.shape[1], X.shape[2]))\n",
        "    L1 = LSTM(4 * latent_dim, activation='relu', return_sequences=True)(inputs)\n",
        "    L2 = LSTM(2*latent_dim, activation='relu', return_sequences=True)(L1)\n",
        "    L3 = LSTM(latent_dim, activation='relu', return_sequences=False)(L2)\n",
        "    L4 = RepeatVector(X.shape[1])(L3)\n",
        "    L5 = LSTM(latent_dim, activation='relu', return_sequences=True)(L4)\n",
        "    L6 = LSTM(2*latent_dim, activation='relu', return_sequences=True)(L5)\n",
        "    L7 = LSTM(4*latent_dim, activation='relu', return_sequences=True)(L6)\n",
        "    output = TimeDistributed(Dense(X.shape[2]))(L7)\n",
        "    model = Model(inputs=inputs, outputs=output)\n",
        "    return model\n",
        "\n",
        "\n",
        "def create_lstm_autoencoder(input_dim, timesteps, latent_dim):\n",
        "    inputs = Input(shape=(timesteps, input_dim,))\n",
        "    encoded = LSTM(latent_dim)(inputs)\n",
        "    decoded = RepeatVector(timesteps)(encoded)\n",
        "    decoded = LSTM(input_dim, return_sequences=True)(decoded)\n",
        "    # sequence_autoencoder = Model(inputs, decoded)\n",
        "    # encoder = Model(inputs, encoded)\n",
        "    autoencoder = Model(inputs, decoded)\n",
        "    autoencoder.compile(optimizer='adam', loss='mse')\n",
        "    return autoencoder\n",
        "\n",
        "\n",
        "def encoder(encoder_inputs):\n",
        "    x = LSTM(latent_dim)(encoder_inputs)\n",
        "    return x\n",
        "\n",
        "def decoder(x):\n",
        "    x = RepeatVector(timesteps)(x)\n",
        "    x = LSTM(input_dim, return_sequences=True)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "#Su dung autoencoder\n",
        "# optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
        "# input = Input(shape=(timesteps, input_dim,))\n",
        "# endcoder_output = encoder(input)\n",
        "# decoder_outputs = decoder(endcoder_output)\n",
        "# model = Model(input, [decoder_outputs])\n",
        "# model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "# model.summary()\n",
        "# model.fit(x_train, x_train, epochs=10000)\n",
        "\n",
        "# Su dung create_lstm_autoencoder\n",
        "\n",
        "model = autoencoder_model1(x_train)\n",
        "#model = create_lstm_autoencoder(input_dim, timesteps, latent_dim)\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "\n",
        "model.fit(x_train, x_train, batch_size=batch_size,epochs=EPOCHS)\n",
        "\n",
        "x_train_pred = model.predict(x_train, batch_size=batch_size)\n",
        "\n",
        "\n",
        "threshold =[]\n",
        "for i in range(0, len(x_train)):\n",
        "    dist = np.sum(np.square(x_train_pred[i] - x_train[i]))\n",
        "    #dist = np.linalg.norm(x_train_pred[i] - x_train[i])\n",
        "    threshold.append(dist)\n",
        "\n",
        "threshold = np.max(threshold)\n",
        "print('threshold:',threshold)\n",
        "x_test_pred = model.predict(x_test, batch_size=batch_size)\n",
        "threshold_test =[]\n",
        "\n",
        "for i in range(0, len(x_test)):\n",
        "    dist = np.sum(np.square(x_test_pred[i] - x_test[i]))\n",
        "    #dist = np.linalg.norm(x_test_pred[i] - x_test[i])\n",
        "    threshold_test.append(dist)\n",
        "\n",
        "anomalies = []\n",
        "\n",
        "for i in range(0, len(threshold_test)):\n",
        "    if threshold_test[i] > threshold:\n",
        "        anomalies.append(1)\n",
        "    else:\n",
        "        anomalies.append(0)\n",
        "\n",
        "print('Number Test Samples:',len(anomalies))\n",
        "print('Number of anomaly samples:', np.sum(anomalies))\n",
        "\n",
        "print(anomalies)\n",
        "print(x_test[0])\n",
        "print('----------------')\n",
        "print(x_test_pred[0])\n",
        "print('---------------')\n",
        "\n",
        "print('---------------------')\n",
        "print(x_test[1])\n",
        "print('----------------')\n",
        "print(x_test_pred[1])\n",
        "print('---------------')\n",
        "\n",
        "print(x_test[2])\n",
        "print('----------------')\n",
        "print(x_test_pred[2])\n",
        "print('---------------')\n",
        "\n",
        "\n",
        "\n",
        "# for i in range(0, 5):\n",
        "#\n",
        "#     t1 = x_train[i].flatten()\n",
        "#     t2 = x_train_pred[i].flatten()\n",
        "#     x1 = x_test[i].flatten()\n",
        "#     y1 = x_test_pred[i].flatten()\n",
        "#\n",
        "#     plt.figure(figsize=(18, 10))\n",
        "#\n",
        "#     plt.subplot('141')\n",
        "#     plt.plot(x1, label='test:' + str(i))\n",
        "#     plt.plot(y1, label='pred test:' + str(i))\n",
        "#     plt.grid(True)\n",
        "#     plt.subplot('142')\n",
        "#     plt.plot(t1, label='train:' + str(i))\n",
        "#     plt.plot(t2, label='pred train:' + str(i))\n",
        "#     plt.grid(True)\n",
        "#\n",
        "#     plt.show();\n",
        "\n"
      ],
      "metadata": {
        "id": "S2uhKJkZOHsW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}