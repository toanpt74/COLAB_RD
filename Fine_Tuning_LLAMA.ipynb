{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toanpt74/COLAB_RD/blob/main/Fine_Tuning_LLAMA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://huggingface.co/sentence-transformers/all-mpnet-base-v2\n",
        "# !pip install -U chromadb==0.3.22 langchain==0.0.164 transformers==4.29.0 accelerate==0.19.0 bitsandbytes\n",
        "\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from sentence_transformers import SentenceTransformer\n",
        "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
        "\n",
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "embeddings = model.encode(sentences)\n",
        "print(embeddings)\n",
        "\n",
        "\n",
        "\n",
        "# ///==========================================================================\n",
        "# !pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7\n",
        "# !pip install -U chromadb==0.3.22 langchain==0.0.164\n",
        "# !git clone https://huggingface.co/NumbersStation/nsql-350M\n",
        "\n",
        "import argparse\n",
        "import bitsandbytes as bnb\n",
        "from datasets import load_dataset\n",
        "from functools import partial\n",
        "import os\n",
        "#from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, AutoPeftModelForCausalLM\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed, Trainer, TrainingArguments, BitsAndBytesConfig,DataCollatorForLanguageModeling, DataCollatorForSeq2Seq\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import pipeline\n",
        "\n",
        "from datetime import datetime\n",
        "import sys\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    get_peft_model_state_dict,\n",
        "    prepare_model_for_int8_training,\n",
        "    set_peft_model_state_dict,\n",
        ")\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "\n",
        "def load_jsonl(data_dir):\n",
        "    data_path = Path(data_dir).as_posix()\n",
        "    data = load_dataset(\"json\", data_files=data_path)\n",
        "    return data\n",
        "\n",
        "def save_jsonl(data_dicts, out_path):\n",
        "    with open(out_path, \"w\") as fp:\n",
        "        for data_dict in data_dicts:\n",
        "            fp.write(json.dumps(data_dict) + \"\\n\")\n",
        "\n",
        "def load_data_sql(data_dir: str = \"data_sql\"):\n",
        "    dataset = load_dataset(\"b-mc2/sql-create-context\")\n",
        "    dataset_splits = {\"train\": dataset[\"train\"]}\n",
        "    out_path = Path(data_dir)\n",
        "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    for key, ds in dataset_splits.items():\n",
        "        with open(out_path, \"w\") as f:\n",
        "            for item in ds:\n",
        "                newitem = {\n",
        "                    \"input\": item[\"question\"],\n",
        "                    \"context\": item[\"context\"],\n",
        "                    \"output\": item[\"answer\"],\n",
        "                }\n",
        "                f.write(json.dumps(newitem) + \"\\n\")\n",
        "\n",
        "load_data_sql(data_dir=\"data_sql\")\n",
        "from math import ceil\n",
        "def get_train_val_splits(\n",
        "    data_dir: str = \"data_sql\",\n",
        "    val_ratio: float = 0.1,\n",
        "    seed: int = 42,\n",
        "    shuffle: bool = True,\n",
        "):\n",
        "    data = load_jsonl(data_dir)\n",
        "    num_samples = len(data[\"train\"])\n",
        "    val_set_size = ceil(val_ratio * num_samples)\n",
        "\n",
        "    train_val = data[\"train\"].train_test_split(\n",
        "        test_size=val_set_size, shuffle=shuffle, seed=seed\n",
        "    )\n",
        "    return train_val[\"train\"].shuffle(), train_val[\"test\"].shuffle()\n",
        "\n",
        "raw_train_data, raw_val_data = get_train_val_splits(data_dir=\"data_sql\")\n",
        "save_jsonl(raw_train_data, \"train_data_raw.jsonl\")\n",
        "save_jsonl(raw_val_data, \"val_data_raw.jsonl\")\n",
        "\n",
        "def create_train_data_from_file(data_dir):\n",
        "    data = load_jsonl(data_dir)\n",
        "    return data[\"train\"]\n",
        "data_train = create_train_data_from_file('/content/train_data_raw.jsonl')\n",
        "data_val = create_train_data_from_file('/content/val_data_raw.jsonl')\n",
        "base_model = \"/content/nsql-350M\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    load_in_8bit=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "tokenizer.add_eos_token = True\n",
        "tokenizer.pad_token_id = 0\n",
        "tokenizer.padding_side = \"left\"\n",
        "def tokenize(prompt):\n",
        "    result = tokenizer(\n",
        "        prompt,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=False,\n",
        "        return_tensors=None,\n",
        "    )\n",
        "\n",
        "    # \"self-supervised learning\" means the labels are also the inputs:\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "\n",
        "    return result\n",
        "\n",
        "def generate_and_tokenize_prompt(data_point):\n",
        "    full_prompt =f\"\"\"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
        "You must output the SQL query that answers the question.\n",
        "### Input:\n",
        "{data_point[\"input\"]}\n",
        "### Context:\n",
        "{data_point[\"context\"]}\n",
        "### Response:\n",
        "{data_point[\"output\"]}\n",
        "\"\"\"\n",
        "    return tokenize(full_prompt)\n",
        "tokenized_train_dataset = data_train.map(generate_and_tokenize_prompt)\n",
        "tokenized_val_dataset = data_val.map(generate_and_tokenize_prompt)\n",
        "\n",
        "model.train() # put model back into training mode\n",
        "model = prepare_model_for_int8_training(model)\n",
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\n",
        "    \"q_proj\",\n",
        "    \"k_proj\",\n",
        "    \"v_proj\",\n",
        "    \"o_proj\",\n",
        "],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model = get_peft_model(model, config)\n",
        "batch_size = 128\n",
        "per_device_train_batch_size = 32\n",
        "gradient_accumulation_steps = batch_size // per_device_train_batch_size\n",
        "output_dir = \"sql-code-llama\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "        per_device_train_batch_size=per_device_train_batch_size,\n",
        "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "        warmup_steps=100,\n",
        "        max_steps=400,\n",
        "        learning_rate=3e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=10,\n",
        "        optim=\"adamw_torch\",\n",
        "        evaluation_strategy=\"steps\", # if val_set_size > 0 else \"no\",\n",
        "        save_strategy=\"steps\",\n",
        "        eval_steps=20,\n",
        "        save_steps=20,\n",
        "        output_dir=output_dir,\n",
        "        load_best_model_at_end=False,\n",
        "        group_by_length=True, # group sequences of roughly the same length together to speed up training\n",
        "        #report_to=\"wandb\", # if use_wandb else \"none\",\n",
        "        run_name=f\"codellama-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\", # if use_wandb else None,\n",
        "    )\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_val_dataset,\n",
        "    args=training_args,\n",
        "    data_collator=DataCollatorForSeq2Seq(\n",
        "        tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
        "    ),\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "S2uhKJkZOHsW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}