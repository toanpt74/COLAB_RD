{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toanpt74/COLAB_RD/blob/main/Pandas-Query-Engine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community\n",
        "!pip install llama-index-embeddings-langchain\n",
        "!pip install llama-index-llms-huggingface\n",
        "!pip install llama-index\n",
        "!pip install chromadb\n",
        "!pip install llama-index-vector-stores-chroma\n",
        "!pip install llama-index-retrievers-bm25\n",
        "!pip install -U bitsandbytes transformers peft accelerate trl datasets\n",
        "!pip install  -U langchain\n",
        "!pip install -U chromadb\n",
        "!pip install sentence_transformers\n",
        "!pip install llama-index-llms-langchain\n",
        "!pip install llama_index.experimental\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    )\n",
        "\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.schema import AIMessage, HumanMessage\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import UnstructuredMarkdownLoader, UnstructuredURLLoader\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain, RetrievalQA, ConversationalRetrievalChain\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
        "\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from typing import List, Tuple\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "#from langchain.chains import RetrievalQA\n",
        "#from langchain import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.load import dumps, loads\n",
        "import torch\n",
        "\n",
        "\n",
        "from huggingface_hub import login\n",
        "access_token_read = \"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\"\n",
        "\n",
        "#\"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\"\n",
        "\n",
        "login(token = access_token_read)\n",
        "\n",
        "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quantization_config\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, padding_side=\"left\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
        "generation_config.max_new_tokens = 1024\n",
        "generation_config.temperature = 0.8\n",
        "generation_config.top_p = 0.95\n",
        "generation_config.do_sample = True\n",
        "generation_config.repetition_penalty = 1.15\n",
        "\n",
        "pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=True,\n",
        "    generation_config=generation_config,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "llm = HuggingFacePipeline(pipeline=pipeline)\n",
        "\n",
        "import logging\n",
        "import sys\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "import pandas as pd\n",
        "from llama_index.experimental.query_engine import PandasQueryEngine\n",
        "\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
        "\n",
        "!wget 'https://raw.githubusercontent.com/jerryjliu/llama_index/main/docs/docs/examples/data/csv/titanic_train.csv' -O 'titanic_train.csv'\n",
        "from llama_index.core import Settings\n",
        "Settings.llm=llm\n",
        "df = pd.read_csv(\"/content/titanic_train.csv\")\n",
        "query_engine = PandasQueryEngine(df=df, verbose=True)\n",
        "response = query_engine.query(\n",
        "    \"What is the correlation between survival and age?\",\n",
        ")\n",
        "print(response.metadata[\"pandas_instruction_str\"])"
      ],
      "metadata": {
        "id": "pra6_FAA6Asw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}